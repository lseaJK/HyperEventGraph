#!/usr/bin/env python3
"""
æ”¹è¿›çš„Cortexå·¥ä½œæµ - æ”¯æŒå¯é…ç½®çš„åˆ†å±‚èšç±»
"""

import os
import requests
import json
import uuid
import argparse
from pathlib import Path
import sys
from typing import List, Dict

# Add project root to sys.path
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

from src.core.config_loader import load_config, get_config
from src.core.database_manager import DatabaseManager

def direct_llm_call(prompt, model="deepseek-ai/DeepSeek-V2.5"):
    """ç›´æ¥è°ƒç”¨APIï¼Œç»•è¿‡é¡¹ç›®çš„LLMClient"""
    api_key = os.getenv('SILICONFLOW_API_KEY')
    if not api_key:
        raise ValueError("éœ€è¦SILICONFLOW_API_KEYç¯å¢ƒå˜é‡")
    
    url = "https://api.siliconflow.cn/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 1000,
        "temperature": 0.7
    }
    
    response = requests.post(url, headers=headers, json=data, timeout=30)
    
    if response.status_code == 200:
        result = response.json()
        return result['choices'][0]['message']['content']
    else:
        raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")

def intelligent_clustering(events_to_cluster, batch_size=200, cluster_ratio=0.3):
    """
    æ™ºèƒ½èšç±»é€»è¾‘ - åŸºäºäº‹ä»¶ç±»å‹å’Œæ—¶é—´è¿›è¡Œåˆ†ç»„
    
    Args:
        events_to_cluster: å¾…èšç±»çš„äº‹ä»¶åˆ—è¡¨
        batch_size: æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°
        cluster_ratio: èšç±»æ¯”ä¾‹ï¼ˆæ‰¹æ¬¡å¤§å° * cluster_ratio = ç›®æ ‡æ•…äº‹æ•°é‡ï¼‰
    """
    print(f"ğŸ“Š ä½¿ç”¨æ™ºèƒ½èšç±»ç­–ç•¥ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}, èšç±»æ¯”ä¾‹: {cluster_ratio}")
    
    # ç¬¬ä¸€æ­¥ï¼šæŒ‰äº‹ä»¶ç±»å‹é¢„åˆ†ç»„
    type_groups = {}
    for event in events_to_cluster:
        event_type = event.get('assigned_event_type', 'Other')
        if event_type not in type_groups:
            type_groups[event_type] = []
        type_groups[event_type].append(event)
    
    print(f"ğŸ“‹ é¢„åˆ†ç»„ç»“æœ: {len(type_groups)} ä¸ªäº‹ä»¶ç±»å‹")
    for event_type, events in type_groups.items():
        print(f"  - {event_type}: {len(events)} ä¸ªäº‹ä»¶")
    
    # ç¬¬äºŒæ­¥ï¼šåœ¨æ¯ä¸ªç±»å‹ç»„å†…è¿›è¡Œæ‰¹æ¬¡å¤„ç†
    all_batches = []
    batch_id = 0
    
    for event_type, events in type_groups.items():
        print(f"\nå¤„ç†äº‹ä»¶ç±»å‹: {event_type} ({len(events)} ä¸ªäº‹ä»¶)")
        
        # å°†äº‹ä»¶åˆ†æˆæ‰¹æ¬¡
        for i in range(0, len(events), batch_size):
            batch_events = events[i:i+batch_size]
            batch_info = {
                'batch_id': batch_id,
                'event_type': event_type,
                'events': batch_events,
                'target_stories': max(1, int(len(batch_events) * cluster_ratio))
            }
            all_batches.append(batch_info)
            print(f"  ğŸ“¦ æ‰¹æ¬¡ {batch_id}: {len(batch_events)} ä¸ªäº‹ä»¶ â†’ ç›®æ ‡ {batch_info['target_stories']} ä¸ªæ•…äº‹")
            batch_id += 1
    
    return all_batches

def process_batch_with_llm(batch_events, target_stories, event_type):
    """
    ä½¿ç”¨LLMå¯¹æ‰¹æ¬¡å†…çš„äº‹ä»¶è¿›è¡Œæ™ºèƒ½èšç±»
    """
    if len(batch_events) <= target_stories:
        # å¦‚æœäº‹ä»¶æ•°é‡å°‘äºç›®æ ‡æ•…äº‹æ•°ï¼Œæ¯ä¸ªäº‹ä»¶ç‹¬ç«‹æˆæ•…äº‹
        return [[event] for event in batch_events]
    
    # å‡†å¤‡äº‹ä»¶æ‘˜è¦ç”¨äºLLMåˆ†æ
    event_summaries = []
    for i, event in enumerate(batch_events):
        summary = {
            'index': i,
            'text': event['source_text'][:200],
            'type': event.get('assigned_event_type', 'Unknown'),
            'entities': event.get('involved_entities', '[]')
        }
        event_summaries.append(summary)
    
    # æ„å»ºLLMæç¤ºè¯
    events_text = "\n".join([
        f"{i}. {summary['text']}" 
        for i, summary in enumerate(event_summaries)
    ])
    
    prompt = f"""
è¯·å°†ä»¥ä¸‹{len(batch_events)}ä¸ª{event_type}ç±»å‹çš„äº‹ä»¶åˆ†æˆ{target_stories}ä¸ªç›¸å…³çš„æ•…äº‹ç»„ã€‚
æ¯ä¸ªæ•…äº‹ç»„åº”è¯¥åŒ…å«é€»è¾‘ç›¸å…³ã€ä¸»é¢˜ç›¸ä¼¼æˆ–æ—¶é—´ç›¸è¿‘çš„äº‹ä»¶ã€‚

äº‹ä»¶åˆ—è¡¨ï¼š
{events_text}

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†ç»„ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "groups": [
    {{"story_id": 1, "event_indices": [0, 1, 5], "reason": "è¿™äº›äº‹ä»¶éƒ½æ¶‰åŠåŒä¸€å®¶å…¬å¸çš„ä¸šåŠ¡å˜åŒ–"}},
    {{"story_id": 2, "event_indices": [2, 3], "reason": "è¿™äº›äº‹ä»¶éƒ½æ˜¯è¡Œä¸šè¶‹åŠ¿ç›¸å…³"}}
  ]
}}

ç¡®ä¿æ‰€æœ‰äº‹ä»¶ç´¢å¼•ï¼ˆ0-{len(batch_events)-1}ï¼‰éƒ½è¢«åˆ†é…åˆ°æŸä¸ªç»„ä¸­ã€‚
"""
    
    try:
        response = direct_llm_call(prompt)
        # å°è¯•è§£æJSON
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            groups = result.get('groups', [])
            
            # éªŒè¯åˆ†ç»„ç»“æœ
            story_groups = []
            used_indices = set()
            
            for group in groups:
                event_indices = group.get('event_indices', [])
                valid_indices = [i for i in event_indices if 0 <= i < len(batch_events) and i not in used_indices]
                if valid_indices:
                    story_group = [batch_events[i] for i in valid_indices]
                    story_groups.append(story_group)
                    used_indices.update(valid_indices)
            
            # å¤„ç†æœªåˆ†é…çš„äº‹ä»¶
            unassigned_indices = set(range(len(batch_events))) - used_indices
            for idx in unassigned_indices:
                story_groups.append([batch_events[idx]])
            
            return story_groups
        
    except Exception as e:
        print(f"âš ï¸ LLMèšç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€å•ç­–ç•¥: {e}")
    
    # å›é€€åˆ°ç®€å•èšç±»ç­–ç•¥
    simple_groups = []
    events_per_group = max(1, len(batch_events) // target_stories)
    
    for i in range(0, len(batch_events), events_per_group):
        group = batch_events[i:i+events_per_group]
        simple_groups.append(group)
    
    return simple_groups

def run_improved_cortex_workflow(batch_size=200, cluster_ratio=0.3):
    """æ”¹è¿›çš„Cortexå·¥ä½œæµä¸»å‡½æ•°"""
    print(f"\n--- Running Improved Cortex Workflow ---")
    print(f"ğŸ“Š å‚æ•°è®¾ç½®: æ‰¹æ¬¡å¤§å°={batch_size}, èšç±»æ¯”ä¾‹={cluster_ratio}")
    
    # 0. Load configuration first
    config_path = project_root / "config.yaml"
    load_config(config_path)
    print("âœ… Configuration loaded successfully")
    
    # 1. Initialization
    config = get_config()
    db_path = config.get('database', {}).get('path')
    db_manager = DatabaseManager(db_path)

    # 2. Fetch pending events
    print("Fetching events pending clustering from the database...")
    events_to_cluster = db_manager.get_records_by_status_as_df('pending_clustering').to_dict('records')
    
    if not events_to_cluster:
        print("No events found pending clustering. Workflow complete.")
        return

    print(f"Found {len(events_to_cluster)} events to process.")

    # 3. Perform intelligent clustering
    print("Performing intelligent clustering...")
    batches = intelligent_clustering(events_to_cluster, batch_size, cluster_ratio)
    
    # 4. Process each batch
    all_stories = []
    processed_events = 0
    
    for batch_info in batches:
        batch_id = batch_info['batch_id']
        event_type = batch_info['event_type']
        batch_events = batch_info['events']
        target_stories = batch_info['target_stories']
        
        print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_id} ({event_type}): {len(batch_events)} ä¸ªäº‹ä»¶ â†’ {target_stories} ä¸ªæ•…äº‹")
        
        # ä½¿ç”¨LLMè¿›è¡Œæ‰¹æ¬¡å†…èšç±»
        story_groups = process_batch_with_llm(batch_events, target_stories, event_type)
        
        # ä¸ºæ¯ä¸ªæ•…äº‹ç»„åˆ›å»ºæ•…äº‹è®°å½•
        for group_idx, story_events in enumerate(story_groups):
            try:
                story_id = f"story_{uuid.uuid4().hex[:8]}"
                
                # ç”Ÿæˆæ•…äº‹æ‘˜è¦
                story_texts = [event['source_text'][:150] for event in story_events]
                combined_text = " | ".join(story_texts)
                
                prompt = f"""
è¯·ä¸ºä»¥ä¸‹{len(story_events)}ä¸ªç›¸å…³çš„{event_type}äº‹ä»¶ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ•…äº‹æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰ï¼š

{combined_text}

è¯·ç›´æ¥å›å¤æ‘˜è¦å†…å®¹ï¼Œçªå‡ºäº‹ä»¶çš„å…±åŒä¸»é¢˜å’Œå…³é”®ä¿¡æ¯ã€‚
"""
                
                summary = direct_llm_call(prompt)
                
                # åˆ›å»ºæ•…äº‹è®°å½•
                story = {
                    'story_id': story_id,
                    'event_ids': [event['id'] for event in story_events],
                    'summary': summary[:200],
                    'event_type': event_type,
                    'batch_id': batch_id
                }
                all_stories.append(story)
                processed_events += len(story_events)
                
                print(f"  ğŸ“ æ•…äº‹ {story_id}: {len(story_events)} ä¸ªäº‹ä»¶")
                print(f"     æ‘˜è¦: {summary[:80]}...")
                
            except Exception as e:
                print(f"  âŒ æ•…äº‹ç”Ÿæˆå¤±è´¥: {e}")
                continue

    # 5. Update database with story information
    if not all_stories:
        print("\nNo stories were generated. No database updates to perform.")
    else:
        print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡: {len(all_stories)} ä¸ªæ•…äº‹ï¼Œè¦†ç›– {processed_events} ä¸ªäº‹ä»¶")
        print("Updating database...")
        
        successful_updates = 0
        for i, story in enumerate(all_stories):
            story_id = story['story_id']
            event_ids_in_story = story['event_ids']
            
            try:
                db_manager.update_story_info(event_ids_in_story, story_id, 'pending_relationship_analysis')
                successful_updates += 1
                if i % 10 == 0:  # æ¯10ä¸ªæ•…äº‹æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print(f"  âœ… å·²æ›´æ–° {i+1}/{len(all_stories)} ä¸ªæ•…äº‹")
            except Exception as e:
                print(f"  âŒ æ•…äº‹ {story_id} æ›´æ–°å¤±è´¥: {e}")
        
        print(f"\nâœ… Database updateå®Œæˆ: {successful_updates}/{len(all_stories)} æ•…äº‹æˆåŠŸæ›´æ–°")

    print("\n--- Improved Cortex Workflow Finished ---")
    print("\n--- èšç±»ç»Ÿè®¡ ---")
    print(f"æ€»äº‹ä»¶æ•°: {len(events_to_cluster)}")
    print(f"æˆåŠŸå¤„ç†: {processed_events}")
    print(f"ç”Ÿæˆæ•…äº‹: {len(all_stories)}")
    print(f"å¹³å‡æ¯æ•…äº‹äº‹ä»¶æ•°: {processed_events / len(all_stories) if all_stories else 0:.1f}")
    print("--------------------------\n")

def main():
    """Entry point with argument parsing."""
    parser = argparse.ArgumentParser(description="æ”¹è¿›çš„Cortexèšç±»å·¥ä½œæµ")
    parser.add_argument("--batch_size", type=int, default=200, 
                       help="æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å° (é»˜è®¤: 200)")
    parser.add_argument("--cluster_ratio", type=float, default=0.3,
                       help="èšç±»æ¯”ä¾‹ï¼Œæ§åˆ¶æ•…äº‹å¯†åº¦ (é»˜è®¤: 0.3)")
    
    args = parser.parse_args()
    
    print("Initializing improved Cortex workflow...")
    try:
        run_improved_cortex_workflow(args.batch_size, args.cluster_ratio)
        print("ğŸ‰ æ”¹è¿›ç‰ˆCortexå·¥ä½œæµå®Œæˆ!")
    except Exception as e:
        print(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
