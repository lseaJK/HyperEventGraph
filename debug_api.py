#!/usr/bin/env python3
"""
è¯¦ç»†çš„APIå‚æ•°è°ƒè¯•è„šæœ¬
"""
import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

from src.core.config_loader import load_config, get_config
from src.llm.llm_client import LLMClient

def debug_config():
    """è°ƒè¯•é…ç½®å†…å®¹"""
    try:
        config_path = project_root / "config.yaml"
        load_config(config_path)
        config = get_config()
        
        print("ğŸ” å½“å‰LLMé…ç½®:")
        llm_config = config.get('llm', {})
        
        # æ˜¾ç¤ºæ¨¡å‹é…ç½®
        models = llm_config.get('models', {})
        for task_type, model_config in models.items():
            print(f"\n{task_type}:")
            for key, value in model_config.items():
                print(f"  {key}: {value}")
        
        return config
    except Exception as e:
        print(f"âŒ é…ç½®è°ƒè¯•å¤±è´¥: {e}")
        return None

async def debug_api_call():
    """è¯¦ç»†è°ƒè¯•APIè°ƒç”¨è¿‡ç¨‹"""
    try:
        llm_client = LLMClient()
        
        # æ‰‹åŠ¨æ„é€ ç®€å•çš„APIè°ƒç”¨å‚æ•°
        messages = [{"role": "user", "content": "hi"}]
        
        print("\nğŸ” è°ƒè¯•APIè°ƒç”¨è¿‡ç¨‹...")
        
        # è·å–triageä»»åŠ¡çš„é…ç½®
        config = get_config()
        triage_config = config['llm']['models']['triage']
        
        print(f"ğŸ“‹ triageé…ç½®: {triage_config}")
        
        # æ‰‹åŠ¨è°ƒç”¨å†…éƒ¨æ–¹æ³•æ¥æŸ¥çœ‹å…·ä½“å‚æ•°
        print("\nğŸ“¤ å‡†å¤‡APIè¯·æ±‚å‚æ•°...")
        
        # æ¨¡æ‹ŸLLMClientå†…éƒ¨çš„å‚æ•°å¤„ç†
        call_params = {
            "model": triage_config['name'],
            "messages": messages,
            "temperature": triage_config.get('temperature', 0.7),
            "max_tokens": triage_config.get('max_tokens', 1024)
        }
        
        print(f"ğŸ”§ å®é™…APIå‚æ•°: {json.dumps(call_params, indent=2)}")
        
        # å°è¯•APIè°ƒç”¨
        client = llm_client._get_client_for_provider('siliconflow')
        
        print(f"\nğŸ“¡ å‘é€è¯·æ±‚åˆ°: {client.base_url}")
        
        response = await client.chat.completions.create(**call_params)
        content = response.choices[0].message.content
        
        print(f"âœ… APIè°ƒç”¨æˆåŠŸ: {content}")
        return True
        
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        
        # å¦‚æœæ˜¯APIé”™è¯¯ï¼Œå°è¯•æ‰“å°æ›´å¤šç»†èŠ‚
        if hasattr(e, 'response'):
            print(f"å“åº”çŠ¶æ€ç : {e.response.status_code}")
            print(f"å“åº”å†…å®¹: {e.response.text}")
        
        import traceback
        traceback.print_exc()
        return False

async def test_different_models():
    """æµ‹è¯•ä¸åŒçš„æ¨¡å‹"""
    models_to_test = [
        "deepseek-ai/DeepSeek-V2.5",
        "deepseek-ai/DeepSeek-V3", 
        "deepseek-ai/DeepSeek-R1"
    ]
    
    for model_name in models_to_test:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
        
        try:
            llm_client = LLMClient()
            client = llm_client._get_client_for_provider('siliconflow')
            
            # æœ€ç®€å‚æ•°
            params = {
                "model": model_name,
                "messages": [{"role": "user", "content": "hi"}],
                "max_tokens": 10
            }
            
            response = await client.chat.completions.create(**params)
            content = response.choices[0].message.content
            print(f"âœ… {model_name}: {content}")
            
        except Exception as e:
            print(f"âŒ {model_name}: {e}")

async def main():
    print("ğŸ”§ è¯¦ç»†APIè°ƒè¯•å¼€å§‹\n")
    
    # è°ƒè¯•é…ç½®
    config = debug_config()
    if not config:
        return
    
    # è°ƒè¯•APIè°ƒç”¨
    print("\n" + "="*50)
    await debug_api_call()
    
    # æµ‹è¯•ä¸åŒæ¨¡å‹
    print("\n" + "="*50)
    await test_different_models()

if __name__ == "__main__":
    asyncio.run(main())
