#!/usr/bin/env python3
"""
Enhanced Cortex Workflow - å¢å¼ºç‰ˆæ™ºèƒ½èšç±»å·¥ä½œæµ
ä½¿ç”¨æ–°çš„å¤šç»´åº¦èšç±»ç®—æ³•æ›¿ä»£ç®€å•çš„åŸºäºç±»å‹çš„åˆ†ç»„

ç‰¹æ€§ï¼š
1. æ—¶é—´-å®ä½“-è¯­ä¹‰-ç±»å‹å¤šç»´åº¦ç‰¹å¾
2. DBSCAN + åå¤„ç†çš„å±‚æ¬¡åŒ–èšç±»
3. æ™ºèƒ½æ•…äº‹æ‘˜è¦ç”Ÿæˆ
4. å¯é…ç½®çš„æƒé‡å’Œå‚æ•°
"""

import os
import json
import uuid
import asyncio
import requests
from pathlib import Path
import sys
from typing import List, Dict

# Add project root to sys.path
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

from src.core.config_loader import load_config, get_config
from src.core.database_manager import DatabaseManager
from src.agents.enhanced_cortex_agent import EnhancedCortexAgent

def direct_llm_call(prompt, model="deepseek-ai/DeepSeek-V2.5"):
    """ç›´æ¥è°ƒç”¨APIè¿›è¡Œæ•…äº‹æ‘˜è¦ç”Ÿæˆ"""
    api_key = os.getenv('SILICONFLOW_API_KEY')
    if not api_key:
        raise ValueError("éœ€è¦SILICONFLOW_API_KEYç¯å¢ƒå˜é‡")
    
    url = "https://api.siliconflow.cn/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 1000,
        "temperature": 0.7
    }
    
    response = requests.post(url, headers=headers, json=data, timeout=30)
    
    if response.status_code == 200:
        result = response.json()
        return result['choices'][0]['message']['content']
    else:
        raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")

def enhanced_cortex_workflow():
    """å¢å¼ºç‰ˆ Cortex å·¥ä½œæµä¸»å‡½æ•°"""
    print("ğŸ§  å¯åŠ¨å¢å¼ºç‰ˆæ™ºèƒ½èšç±»å·¥ä½œæµ...")
    
    # 1. åŠ è½½é…ç½®å’Œåˆå§‹åŒ– (use explicit project-root config path to avoid ambiguity)
    config_path = project_root / "config.yaml"
    load_config(config_path)
    config = get_config()
    print(f"Configuration loaded from: {config_path}")
    db_manager = DatabaseManager(config['database']['path'])
    
    print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    # debug: show database path used
    db_path_debug = config.get('database', {}).get('path')
    print(f"Using database path: {db_path_debug}")
    
    # 2. è·å–å¾…èšç±»äº‹ä»¶
    pending_events_df = db_manager.get_records_by_status_as_df('pending_clustering')
    
    if pending_events_df.empty:
        print("â„¹ï¸ æ²¡æœ‰å¾…èšç±»çš„äº‹ä»¶")
        return
    
    print(f"ğŸ“Š å‘ç° {len(pending_events_df)} ä¸ªå¾…èšç±»äº‹ä»¶")
    
    # æ£€æŸ¥æµ‹è¯•é™åˆ¶
    test_limit = os.getenv('TEST_LIMIT')
    if test_limit:
        try:
            limit = int(test_limit)
            if len(pending_events_df) > limit:
                pending_events_df = pending_events_df.head(limit)
                print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: é™åˆ¶å¤„ç† {limit} ä¸ªäº‹ä»¶")
        except ValueError:
            print("âš ï¸ TEST_LIMIT ç¯å¢ƒå˜é‡æ— æ•ˆï¼Œå¿½ç•¥é™åˆ¶")
    
    # 3. åˆå§‹åŒ–å¢å¼ºç‰ˆ Cortex ä»£ç†
    cortex_agent = EnhancedCortexAgent()
    
    # 4. å‡†å¤‡äº‹ä»¶æ•°æ®
    events = []
    for _, row in pending_events_df.iterrows():
        event_data = {
            'id': row['id'],
            'source_text': row['source_text'],
            'assigned_event_type': row.get('assigned_event_type', ''),
            'structured_data': row.get('structured_data', ''),
            'involved_entities': row.get('involved_entities', ''),
            'event_date': '',  # éœ€è¦ä» structured_data ä¸­æå–
        }
        
        # ä» structured_data ä¸­æå–æ—¥æœŸ
        if event_data['structured_data']:
            try:
                structured = json.loads(event_data['structured_data'])
                event_data['event_date'] = structured.get('event_date', '')
                event_data['micro_event_type'] = structured.get('micro_event_type', '')
            except:
                pass
        
        events.append(event_data)
    
    # 5. æ‰§è¡Œæ™ºèƒ½èšç±»
    print("ğŸ” å¼€å§‹æ™ºèƒ½å¤šç»´åº¦èšç±»...")
    clusters = cortex_agent.intelligent_clustering(events)
    
    if not clusters:
        print("âš ï¸ æœªå‘ç°ä»»ä½•èšç±»")
        return
    
    print(f"âœ… èšç±»å®Œæˆï¼Œå‘ç° {len(clusters)} ä¸ªæ•…äº‹ç°‡")
    
    # 6. ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆæ•…äº‹
    stories = []
    
    for i, cluster_indices in enumerate(clusters):
        print(f"\n--- å¤„ç†ç°‡ #{i+1} ({len(cluster_indices)} ä¸ªäº‹ä»¶) ---")
        
        # ç”Ÿæˆæ•…äº‹ID
        story_id = f"story_{uuid.uuid4().hex[:8]}"
        
        # ç”Ÿæˆå¢å¼ºæ•…äº‹æ‘˜è¦
        enhanced_summary = cortex_agent.generate_enhanced_story_summary(cluster_indices, events)
        print(f"ğŸ“ å¢å¼ºæ‘˜è¦: {enhanced_summary}")
        
        # å‡†å¤‡äº‹ä»¶æ–‡æœ¬ç”¨äºLLMæ‘˜è¦ - é™åˆ¶å¤§å°é¿å…APIé”™è¯¯
        cluster_texts = []
        cluster_event_ids = []
        max_events_for_llm = 50  # é™åˆ¶LLMå¤„ç†çš„äº‹ä»¶æ•°é‡
        
        for idx in cluster_indices[:max_events_for_llm]:  # åªå–å‰50ä¸ªäº‹ä»¶
            event = events[idx]
            cluster_event_ids.append(event['id'])
            
            # æ„å»ºäº‹ä»¶æè¿°
            structured_data = event.get('structured_data', '')
            if structured_data:
                try:
                    structured = json.loads(structured_data)
                    description = structured.get('description', '')
                    if description:
                        cluster_texts.append(f"äº‹ä»¶{idx+1}: {description[:100]}...")  # é™åˆ¶å•ä¸ªäº‹ä»¶é•¿åº¦
                    else:
                        cluster_texts.append(f"äº‹ä»¶{idx+1}: {event['source_text'][:80]}...")
                except:
                    cluster_texts.append(f"äº‹ä»¶{idx+1}: {event['source_text'][:80]}...")
            else:
                cluster_texts.append(f"äº‹ä»¶{idx+1}: {event['source_text'][:80]}...")
        
        # æ‰€æœ‰äº‹ä»¶IDéƒ½è¦æ›´æ–°ï¼Œä¸åªæ˜¯ç”¨äºLLMçš„å‰50ä¸ª
        cluster_event_ids = [events[idx]['id'] for idx in cluster_indices]
        
        # ä½¿ç”¨LLMç”Ÿæˆæ•…äº‹æ‘˜è¦ - æ·»åŠ å¤§ç°‡æç¤º
        events_text = "\\n".join(cluster_texts)
        size_note = f" (æ³¨ï¼šæ­¤ç°‡å…±{len(cluster_indices)}ä¸ªäº‹ä»¶ï¼Œä»¥ä¸‹ä»…å±•ç¤ºå‰{min(len(cluster_indices), max_events_for_llm)}ä¸ª)" if len(cluster_indices) > max_events_for_llm else ""
        
        prompt = f"""è¯·åŸºäºä»¥ä¸‹ç›¸å…³äº‹ä»¶ï¼Œç”Ÿæˆä¸€ä¸ªè¿è´¯çš„æ•…äº‹æ‘˜è¦{size_note}ï¼š

{events_text}

è¦æ±‚ï¼š
1. æ‘˜è¦åº”è¯¥çªå‡ºäº‹ä»¶ä¹‹é—´çš„å…³è”æ€§å’Œé€»è¾‘å…³ç³»
2. æ§åˆ¶åœ¨150å­—ä»¥å†…
3. ä½¿ç”¨ä¸­æ–‡
4. é‡ç‚¹å…³æ³¨å› æœå…³ç³»ã€æ—¶é—´é¡ºåºã€æ¶‰åŠå®ä½“

æ•…äº‹æ‘˜è¦ï¼š"""
        
        try:
            llm_summary = direct_llm_call(prompt)
            print(f"ğŸ“ LLMæ‘˜è¦: {llm_summary[:100]}...")
        except Exception as e:
            print(f"âš ï¸ LLMæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            llm_summary = enhanced_summary  # ä½¿ç”¨å¢å¼ºæ‘˜è¦ä½œä¸ºå¤‡ç”¨
        
        # ä¿å­˜æ•…äº‹ä¿¡æ¯
        story_info = {
            'story_id': story_id,
            'event_ids': cluster_event_ids,
            'enhanced_summary': enhanced_summary,
            'llm_summary': llm_summary,
            'event_count': len(cluster_indices)
        }
        
        stories.append(story_info)
        print(f"âœ… æ•…äº‹ {story_id} ç”Ÿæˆå®Œæˆ")
    
    # 7. æ›´æ–°æ•°æ®åº“
    print(f"\\nğŸ“Š å¼€å§‹æ›´æ–°æ•°æ®åº“...")
    
    for i, story in enumerate(stories):
        print(f"\\næ›´æ–°æ•…äº‹ {i+1}/{len(stories)}: {story['story_id']}")
        print(f"  åŒ…å«äº‹ä»¶æ•°: {story['event_count']}")
        print(f"  äº‹ä»¶IDç¤ºä¾‹: {story['event_ids'][:3]}")
        
        try:
            # ä½¿ç”¨ DatabaseManager çš„ update_story_info æ–¹æ³•
            db_manager.update_story_info(
                event_ids=story['event_ids'],
                story_id=story['story_id'],
                new_status='story_assigned'
            )
            print(f"  âœ… æˆåŠŸæ›´æ–°")
        except Exception as e:
            print(f"  âŒ æ›´æ–°å¤±è´¥: {e}")
    
    # 8. æœ€ç»ˆç»Ÿè®¡
    print(f"\\nğŸ‰ å¢å¼ºç‰ˆCortexå·¥ä½œæµå®Œæˆ!")
    print(f"\\n--- èšç±»ç»Ÿè®¡ ---")
    print(f"æ€»äº‹ä»¶æ•°: {len(events)}")
    print(f"ç”Ÿæˆæ•…äº‹æ•°: {len(stories)}")
    print(f"å¹³å‡æ¯æ•…äº‹äº‹ä»¶æ•°: {len(events)/len(stories):.1f}")
    
    # æ˜¾ç¤ºæ•…äº‹åˆ†å¸ƒ
    for i, story in enumerate(stories):
        print(f"æ•…äº‹ {i+1}: {story['event_count']} ä¸ªäº‹ä»¶")
    
    return True

if __name__ == "__main__":
    try:
        success = enhanced_cortex_workflow()
        
        if success:
            print("\\nğŸ¯ å»ºè®®ä¸‹ä¸€æ­¥:")
            print("1. æ£€æŸ¥æ•…äº‹è´¨é‡: python check_database_status.py")
            print("2. è¿è¡Œå…³ç³»åˆ†æ: python run_relationship_analysis.py")
            print("3. æŸ¥çœ‹çŸ¥è¯†å›¾è°±: åœ¨Neo4j Browserä¸­æ¢ç´¢")
        
    except KeyboardInterrupt:
        print("\\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\\nâŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
