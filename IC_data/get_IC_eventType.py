import os
import json
import requests
from typing import List, Dict

# ====== é€šç”¨é…ç½® ======
# API_KEY = os.getenv("SILICONFLOW_API_KEY")  # å»ºè®®å†™åˆ°ç¯å¢ƒå˜é‡
API_KEY = "sk-gyfzsxbiliybpcqteeuhdykexrfindhmuhwheuamasqqvdym"
BATCH_SIZE = 20                            # æ¯æ¬¡è¯·æ±‚ 20 æ¡
RESULT_FILE = "deepseek_results.jsonl"     # è¿½åŠ å†™ç»“æœ
PROMPT_PREFIX = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºé›†æˆç”µè·¯ï¼ˆICï¼‰è¡Œä¸šäº‹ä»¶æŠ½å–çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä»»åŠ¡æ˜¯ä»è¾“å…¥çš„æ–°é—»æ–‡æœ¬ä¸­è¯†åˆ«å’Œæå–**äº‹ä»¶ç±»å‹**ï¼Œç”¨äºæ„å»º**äº‹ç†å›¾è°±ä¸­çš„äº‹ä»¶æ¦‚å¿µå±‚æ ‡ç­¾**ã€‚

### ğŸ§  èƒŒæ™¯è¯´æ˜ï¼š

åœ¨äº‹ç†å›¾è°±æ„å»ºè¿‡ç¨‹ä¸­ï¼Œäº‹ä»¶ç±»å‹ï¼ˆEvent Typeï¼‰ç”¨äºå¯¹æ–°é—»æ–‡æœ¬ä¸­å‘ç”Ÿçš„äº‹ä»¶è¿›è¡Œé«˜å±‚æ¬¡ã€**ç»†ç²’åº¦ä¸”è¯­ä¹‰æ¸…æ™°çš„åˆ†ç±»æ ‡æ³¨**ã€‚æ¯ä¸ªäº‹ä»¶ç±»å‹åº”ä»£è¡¨ä¸€ä¸ªæŠ½è±¡ä½†å…·ä½“çš„äº‹ä»¶æ¦‚å¿µï¼Œå¦‚â€œä»·æ ¼è°ƒæ•´â€ã€â€œåˆä½œç­¾ç½²â€ç­‰ã€‚é¿å…ä½¿ç”¨æ¨¡ç³Šæˆ–è¿‡äºå®½æ³›çš„æ ‡ç­¾ï¼ˆå¦‚â€œå•†ä¸šäº‹ä»¶â€æˆ–â€œå¸‚åœºåŠ¨æ€â€ï¼‰ã€‚

### ğŸ¯ ä»»åŠ¡ç›®æ ‡ï¼š

ä»ä¸€ç»„ICé¢†åŸŸçš„æ–°é—»æ–‡æœ¬ä¸­ï¼Œå½’çº³å¹¶è¾“å‡ºæ‰€æœ‰å”¯ä¸€çš„ã€å…·ä½“çš„**äº‹ä»¶ç±»å‹æ ‡ç­¾**ï¼Œä»¥ä¸­æ–‡ä¸ºä¸»ï¼Œè¾“å‡ºæ ¼å¼ä¸º JSON æ•°ç»„ã€‚

### ğŸ“¥ è¾“å…¥è¯´æ˜ï¼š

- è¾“å…¥æ˜¯ä¸€ç»„ä¸é›†æˆç”µè·¯äº§ä¸šç›¸å…³çš„æ–°é—»æ–‡æœ¬ï¼Œæ¯æ¡ä¸ºä¸€ä¸ªæ®µè½æˆ–å¥å­ã€‚
  
- æ¯æ¡æ–‡æœ¬å¯èƒ½åŒ…å«ï¼šæ—¶é—´ã€ä¸»ä½“ï¼ˆå¦‚å…¬å¸ã€æ”¿åºœæœºæ„ï¼‰ã€è¡Œä¸ºï¼ˆå¦‚åˆä½œã€ç ”å‘ã€èèµ„ç­‰ï¼‰ä»¥åŠå½±å“æˆ–ç»“æœã€‚

### ğŸ“¤ è¾“å‡ºè¦æ±‚ï¼š

- è¾“å‡ºä¸€ä¸ª JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå…·ä½“ã€å”¯ä¸€çš„**äº‹ä»¶ç±»å‹æ ‡ç­¾**ï¼ˆä¼˜å…ˆä½¿ç”¨ä¸­æ–‡ï¼‰ã€‚
  
- æ¯ä¸ªæ ‡ç­¾åº”ï¼š
  
  - ç²¾ç‚¼å…·ä½“ï¼Œé•¿åº¦å»ºè®®ä¸è¶…è¿‡10ä¸ªæ±‰å­—ï¼›
    
  - è¡¨ç¤ºä¸€ä¸ªæŠ½è±¡ä½†å¯è¯†åˆ«çš„äº‹ä»¶ç±»å‹ï¼›
    
  - ä¸åŒ…å«ç‰¹å®šå…¬å¸åã€æ•°å­—æˆ–éç±»å‹æ€§å†…å®¹ã€‚
    
- è¾“å‡ºä»…åŒ…æ‹¬äº‹ä»¶ç±»å‹æ ‡ç­¾ï¼Œä¸è¾“å‡ºäº‹ä»¶åŸæ–‡æˆ–å…¶ä»–è§£é‡Šæ€§å†…å®¹ã€‚
  

```json
["äº‹ä»¶ç±»å‹1", "äº‹ä»¶ç±»å‹2", "äº‹ä»¶ç±»å‹3", ...]
```

### âœ… æ‰§è¡Œæ­¥éª¤ï¼ˆå†…éƒ¨å¤„ç†é€»è¾‘ï¼Œæ— éœ€è¾“å‡ºï¼‰ï¼š

1. **äº‹ä»¶è¯†åˆ«**ï¼šæå–æ¯æ¡æ–‡æœ¬çš„å…³é”®è¦ç´ ï¼ˆä¸»ä½“ã€è¡Œä¸ºã€å¯¹è±¡ã€å½±å“ç­‰ï¼‰ã€‚
  
2. **ç±»å‹å½’ç±»**ï¼šå°†æ¯ä¸ªäº‹ä»¶å½’å…¥æœ€è´´åˆ‡çš„ç»†ç²’åº¦ç±»å‹æ ‡ç­¾ã€‚
  
3. **åŒä¹‰åˆå¹¶**ï¼šå°†è¯­ä¹‰æ¥è¿‘çš„äº‹ä»¶ï¼ˆå¦‚â€œé™ä»·â€ã€â€œä»·æ ¼ä¸‹è°ƒâ€ï¼‰ç»Ÿä¸€ä¸ºä¸€ä¸ªæ ‡å‡†ç±»å‹ï¼ˆå¦‚â€œä»·æ ¼è°ƒæ•´â€ï¼‰ã€‚
  
4. **å»é‡æ•´ç†**ï¼šè¾“å‡ºå”¯ä¸€ã€å®Œæ•´ã€æ— é—æ¼çš„äº‹ä»¶ç±»å‹é›†åˆã€‚

---

ç°åœ¨ï¼Œè¯·åŸºäºç”¨æˆ·æä¾›çš„å®é™…æ–‡æœ¬ç»„ï¼Œè¾“å‡ºäº‹ä»¶ç±»å‹åˆ—è¡¨ã€‚

"""

# ====== è¯»å– filtered_data ======
def load_filtered_data(path: str = "filtered_data.json") -> List[str]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# ====== è°ƒç”¨ SiliconFlow DeepSeek ======

import requests

url = "https://api.siliconflow.cn/v1/chat/completions"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}


def ask_deepseek(batch: List[str]) -> str:
    """
    æŠŠ 20 æ¡æ–‡æœ¬åˆå¹¶æˆä¸€æ¡ prompt å‘é€ç»™ DeepSeekï¼Œè¿”å›æ¨¡å‹å›å¤ã€‚
    """
    content = PROMPT_PREFIX + "\n".join(f"{i+1}. {q}" for i, q in enumerate(batch))
#     print(content)
#     exit(0)
    payload = {
        "model": "deepseek-ai/DeepSeek-R1",
        "messages": [{"role": "user", "content": content}],
        "temperature": 0.3,
        "max_tokens": 4096,
        "stream": False
    }
    resp = requests.request("POST", url, json=payload, headers=headers)
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"].strip()

# # ====== ä¸»æµç¨‹ ======
# def iterate_and_ask():
#     data = load_filtered_data()
#     total = len(data)
#     for start in range(0, total, BATCH_SIZE):
#         end = min(start + BATCH_SIZE, total)
#         batch = data[start:end]
#         print(f"[{start}-{end-1}] æ­£åœ¨è¯·æ±‚ DeepSeek â€¦")
#         try:
#             answer = ask_deepseek(batch)
#             result = {
#                 "batch_index": start // BATCH_SIZE,
#                 "questions": batch,
#                 "answer": answer
#             }
#             # è¿½åŠ å†™ jsonl
#             with open(RESULT_FILE, "a", encoding="utf-8") as f:
#                 f.write(json.dumps(result, ensure_ascii=False) + "\n")
#             print(f"å·²å†™å…¥ç¬¬ {start//BATCH_SIZE} æ‰¹ç»“æœ")
#             break
#         except Exception as e:
#             print(f"ç¬¬ {start//BATCH_SIZE} æ‰¹å‡ºé”™ï¼š{e}")
#             continue

# if __name__ == "__main__":
#     # ä¿è¯ä¸ä¼šé‡å¤å†™å…¥ï¼Œå¯å…ˆæ¸…ç©ºç»“æœæ–‡ä»¶
#     if os.path.exists(RESULT_FILE):
#         os.remove(RESULT_FILE)
#     iterate_and_ask()
    
import os
import json
import threading
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

BATCH_SIZE = 10
NUM_WORKERS = 10
RETRY_WORKERS = 5
RESULT_FILE = "result.jsonl"
LOCK = threading.Lock()


# æ”¾åœ¨æ–‡ä»¶å¤´éƒ¨
from ratelimit import limits, sleep_and_retry
RPM = 1000            # æ¯åˆ†é’Ÿæœ€å¤š 1000 æ¬¡

# æŠŠçœŸæ­£çš„è¯·æ±‚å‡½æ•°å†åŒ…ä¸€å±‚
@sleep_and_retry
@limits(calls=RPM, period=60)
def ask_deepseek_rl(batch):
    # è¿™é‡Œè¿˜æ˜¯åŸæ¥çš„é€»è¾‘ï¼Œåªæ˜¯åå­—æ¢ä¸€ä¸‹ï¼Œæ–¹ä¾¿æ’å…¥é™æµ
    return ask_deepseek(batch)

def read_completed_batches():
    if not os.path.exists(RESULT_FILE):
        return set()
    completed = set()
    with open(RESULT_FILE, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line.strip())
                completed.add(obj["batch_index"])
            except:
                continue
    return completed

def process_batch(batch_index, data):
    start = batch_index * BATCH_SIZE
    end = min(start + BATCH_SIZE, len(data))
    batch = data[start:end]
    try:
        print(f"[{start}-{end - 1}] æ­£åœ¨è¯·æ±‚ DeepSeek â€¦")
        answer = ask_deepseek_rl(batch)
        result = {
            "batch_index": batch_index,
#             "questions": batch,
            "answer": answer
        }
        with LOCK:
            with open(RESULT_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps(result, ensure_ascii=False) + "\n")
        print(f"âœ… å·²å†™å…¥ç¬¬ {batch_index} æ‰¹ç»“æœ")
        return True
    except Exception as e:
        print(f"âŒ ç¬¬ {batch_index} æ‰¹å‡ºé”™ï¼š{e}")
        return False

def run_batches(batch_indices, data, num_workers):
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = []
        for idx in batch_indices:
            futures.append(executor.submit(process_batch, idx, data))
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = [f.result() for f in tqdm(futures, desc="æ‰§è¡Œæ‰¹æ¬¡")]
    return results

def main():
    data = load_filtered_data()
    total_batches = (len(data) + BATCH_SIZE - 1) // BATCH_SIZE

    # è¯»å–å·²ç»å®Œæˆçš„æ‰¹æ¬¡
    completed_batches = read_completed_batches()
    all_batches = set(range(total_batches))
    remaining_batches = sorted(all_batches - completed_batches)

    print(f"ğŸ“Œ æ€»æ‰¹æ¬¡ï¼š{total_batches}ï¼Œå·²å®Œæˆï¼š{len(completed_batches)}ï¼Œå¾…å¤„ç†ï¼š{len(remaining_batches)}")

    # ç¬¬ä¸€è½®å¹¶å‘æ‰§è¡Œ
    if remaining_batches:
        print(f"ğŸš€ å¼€å§‹ç¬¬ä¸€è½®å¹¶å‘æ‰§è¡Œ {len(remaining_batches)} æ‰¹")
        run_batches(remaining_batches, data, NUM_WORKERS)

    # æ£€æŸ¥æ˜¯å¦ä»æœ‰æœªå®Œæˆçš„æ‰¹æ¬¡
    completed_batches = read_completed_batches()
    remaining_batches = sorted(all_batches - completed_batches)

    if remaining_batches:
        print(f"âš ï¸ ç¬¬äºŒè½®é‡è¯•æœªå®Œæˆçš„ {len(remaining_batches)} æ‰¹")
        run_batches(remaining_batches, data, RETRY_WORKERS)

    # æœ€ç»ˆæ£€æŸ¥
    completed_batches = read_completed_batches()
    remaining_batches = sorted(all_batches - completed_batches)

    if not remaining_batches:
        print("ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡æ‰§è¡Œå®Œæ¯•")
    else:
        print(f"â— ä»æœ‰æœªå®Œæˆçš„æ‰¹æ¬¡ï¼š{remaining_batches}")

if __name__ == "__main__":
    main()
