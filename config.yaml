# config.yaml
# This file centralizes all system configurations for the HyperEventGraph project.

# --- Global Model Settings ---
model_settings:
  # Central directory for caching downloaded models (e.g., from HuggingFace)
  cache_dir: "/home/kai/models"

# --- Database Configuration ---
database:
  path: "master_state.db"

# --- LLM Provider and Model Routing Configuration ---
# The system uses a unified LLM client that routes tasks to different models
# based on this configuration. API keys should be set as environment variables.
llm:
  providers:
    siliconflow:
      base_url: "https://api.siliconflow.cn/v1"
      # API key is read from the SILICON_API_KEY environment variable.

  models:
    # Model for initial triage: fast and cost-effective.
    triage:
      provider: "siliconflow"
      name: "deepseek-ai/DeepSeek-V3"
      temperature: 0.7
      max_tokens: 1024

    # Model for schema generation: powerful, large context.
    schema_generation:
      provider: "siliconflow"
      name: "deepseek-ai/DeepSeek-R1"
      temperature: 0.6

    # Model for extraction: strong instruction-following and JSON capabilities.
    extraction:
      provider: "siliconflow"
      name: "deepseek-ai/DeepSeek-V3"
      temperature: 0.3
      max_tokens: 8192

    # Model for relationship analysis: powerful, logical reasoning.
    relationship_analysis:
      provider: "siliconflow"
      name: "deepseek-ai/DeepSeek-R1"
      temperature: 0.2
      max_tokens: 4096

# --- Workflow Specific Configurations ---
review_workflow:
  # Path for the CSV file generated for human review.
  review_csv: "output/review/review_sheet.csv"

learning_workflow:
  # Path to the central JSON file where learned schemas are stored.
  schema_registry_path: "output/schemas/event_schemas.json"
  # Distance threshold for clustering similar texts. Lower is stricter.
  cluster_distance_threshold: 1.4

extraction_workflow:
  # Path for the final output file containing structured event data.
  output_file: "output/extraction/structured_events.jsonl"

# --- Cortex Workflow Configuration ---
cortex:
  # Number of events in 'pending_clustering' state required to auto-trigger the workflow.
  trigger_threshold: 100
  vectorizer:
    model_type: "local" # "local" or "api"
    model_name: "BAAI/bge-large-zh-v1.5"
  refinement:
    # Threshold for what constitutes a "large" cluster that needs special handling.
    large_cluster_threshold: 20
  clustering:
    # Enhanced clustering parameters for multi-dimensional approach
    # Weight for entity similarity in the final distance calculation.
    entity_weight: 0.3
    # Weight for semantic similarity (BGE embeddings)
    semantic_weight: 0.4
    # Weight for time-based features
    time_weight: 0.2
    # Weight for event type features
    type_weight: 0.1
    # Time window for grouping events (in days)
    time_window_days: 30
    # DBSCAN `eps` parameter: The maximum distance between two samples for one to be considered as in the neighborhood of the other.
    # Reduced from 0.8 to 0.6 for tighter clustering
    dbscan_eps: 0.6
    # DBSCAN `min_samples` parameter: The number of samples in a neighborhood for a point to be considered as a core point.
    dbscan_min_samples: 2

# --- Storage Configuration ---
storage:
  neo4j:
    uri: "bolt://localhost:7687"
    user: "neo4j"
    password: "neo123456" # IMPORTANT: Please change this to your actual Neo4j password.
  chroma:
    path: "./chroma_db"

# --- Relationship Analysis Workflow Configuration ---
relationship_analysis:
  # Path for logging processed event IDs to support resumability.
  log_file: "output/logs/relationship_analysis.log"
  # Max number of events to send to the LLM in a single prompt.
  chunk_size: 100
  # Path for the JSONL file to store raw LLM outputs for traceability.
  raw_output_file: "output/extraction/relationships_raw.jsonl"
