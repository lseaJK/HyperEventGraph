Clustering evaluation (minimal-cost C)

Purpose:
- Produce a CSV sample for human review that lists a few example events per cluster (by `story_id` or `cluster_id`).
- Produce a JSON report with simple TF-IDF-based cohesion (mean cosine-to-centroid), inter-centroid similarity, and silhouette (cosine) when applicable.

Quick usage:

1. Run evaluation for stories generated by clustering (default `story_id`):

```powershell
python run_clustering_evaluation.py --group-by story_id --status pending_relationship_analysis --sample-per-group 3 --out-dir outputs
```

2. Run evaluation for algorithmic clusters (use `cluster_id`):

```powershell
python run_clustering_evaluation.py --group-by cluster_id --status pending_refinement --sample-per-group 5 --out-dir outputs
```

Outputs written to `outputs/` by default:
- clustering_evaluation_samples_<ts>.csv  (sample rows for human QA)
- clustering_evaluation_report_<ts>.json  (metrics and per-cluster stats)

Notes:
- This script purposely uses TF-IDF (no heavy embedding models) to keep cost low and fast.
- If `jieba` is installed in your environment, it will be used for Chinese tokenization.
- Adjust `--min-group-size` to filter out tiny clusters from metrics.
