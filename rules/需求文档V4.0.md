# HyperEventGraph V3.1: 架构与实施蓝图

**版本**: 3.1 (Architecture & Implementation Blueprint)
**作者**: Gemini Architect
**日期**: 2025-07-27

---

## 1. 核心愿景：构建一个“活”的知识系统

我们的目标是构建一个能够**自我完善、持续迭代**的领域事件知识图谱系统。它不仅能处理一次性的数据输入，更能在一个**闭环的知识循环**中不断学习，扩大认知边界，从而实现真正的“智能”和“进化”。

---

## 2. V3.1 架构核心：中央状态驱动

系统的核心是一个**中央状态数据库 (`master_state.db`)** 和一个**中央配置文件 (`config.yaml`)**。所有工作流脚本都是无状态的，它们通过查询和更新中央数据库来协同工作，由中央配置文件统一驱动。

```
+-----------------------------------------------------------------+
|                          config.yaml                            |
| (Paths, Batch Sizes, Model Names, All System-wide Parameters)   |
+-----------------------------------------------------------------+
                                |
                                v
+-----------------------------------------------------------------+
|                       master_state.db (SQLite)                    |
| (ID, Text, Status, Confidence, Type, Notes, Timestamp)          |
+-----------------------------------------------------------------+
     ^       |           ^       |           ^       |           ^
     |       |           |       |           |       |           |
     |       v           |       v           |       v           |
+----|-------+----+ +----|-------+----+ +----|-------+----+ +----|-------+----+
| Task #13:      | | Task #14:      | | Task #15:      | | Task #16:      |
| Batch Triage   | | Human Review   | | Schema Learning| | Extraction     |
| (State: -> PM) | | (State: -> PL/PE)| | (State: -> PT) | | (State: -> C)  |
+----------------+ +----------------+ +----------------+ +----------------+
```
*（状态码：PM=Pending Review, PL=Pending Learning, PE=Pending Extraction, PT=Pending Triage, C=Completed）*

---

## 3. 核心模块实施细则 (The "How")

### 3.1. 任务 #12: 奠定基石 - 建立中央状态库与配置文件

- **目标**: 搭建整个系统的“中央神经系统”。
- **实施要点**:
  - **数据库模块**: 创建 `src/core/database_manager.py`。
    - 使用 `sqlite3` 库。
    - 定义 `MasterState` 表，字段: `id TEXT PRIMARY KEY`, `source_text TEXT`, `current_status TEXT`, `triage_confidence REAL`, `assigned_event_type TEXT`, `notes TEXT`, `last_updated TIMESTAMP`。
    - 实现所有CRUD函数 (`add_new_texts`, `get_texts_by_status`, `update_text_status` 等)。
    - 此模块将完全取代旧的 `src/workflows/state_manager.py`。
  - **配置模块**: 创建 `src/core/config_loader.py`。
    - 使用 `PyYAML` 库加载和验证 `config.yaml`。
    - 提供全局可用的 `get_config()` 函数。
  - **统一配置文件**: 创建 `config.yaml`，并将所有分散的路径、批次大小、模型名称等参数统一管理。

### 3.2. 任务 #13: 实施第一阶段 - 开发与状态库集成的批量初筛工作流

- **目标**: 高效、稳健地完成海量数据的初步分类。
- **实施要点**:
  - **重构 `run_batch_triage.py`**:
    - 必须导入并使用 `database_manager` 和 `config_loader`。
    - 主循环逻辑:
      1. 从数据库查询状态为 `pending_triage` 的数据，按批次处理。
      2. **修改 `TriageAgent` 的Prompt**，要求其在JSON输出中必须包含一个 `confidence_score` 字段。
      3. 将分类结果（事件类型、置信度）写回数据库，并将状态更新为 `pending_review`。
    - 该脚本的批处理+数据库查询/更新机制，天然地实现了检查点和断点续传。

### 3.3. 任务 #14: 实施第二阶段 - 开发支持优先级的离线人工审核工作流

- **目标**: 建立高效、聚焦的人类智慧网关。
- **实施要点**:
  - **创建 `prepare_review_file.py`**:
    - 从数据库查询所有状态为 `pending_review` 的记录。
    - **核心功能**: 按 `triage_confidence` 升序排序，让审核员优先处理AI最不确定的案例。
    - 生成 `review_sheet.csv` 供人工审核。
  - **创建 `process_review_results.py`**:
    - 读取审核完成的CSV。
    - 将每一行经过人工校准的结果，通过 `database_manager` 更新回数据库，将状态设置为 `pending_learning` 或 `pending_extraction`。

### 3.4. 任务 #15: 实施第三阶段 - 开发增强型交互式学习工作流

- **目标**: 建立一个强大的、由专家引导的知识增长引擎，并闭合迭代循环。
- **实施要点**:
  - **创建 `run_learning_workflow.py`**:
    - 从数据库查询状态为 `pending_learning` 的数据。
    - **创建 `src/agents/toolkits/schema_learning_toolkit.py`**: 封装聚类 (`scikit-learn`)、样本展示 (`pandas`)、LLM调用等核心算法。
    - **实现增强型CLI**: 提供 `list_clusters`, `show_samples <id>`, `merge_clusters <id1> <id2>`, `generate_schema <id>` 等丰富的交互指令。
    - **闭合循环与即时处理**: 这是V4.0对学习工作流的核心增强。当一个新Schema被学会并保存后，工作流将：
        1. **立���再处理**: 自动对该簇内的所有事件，使用新学会的Schema进行一次即时的、目标明确的事件抽取。
        2. **直接入库**: 如果抽取成功，则将完整的结构化事件存入数据库，并将状态更新为 `processed`。
        3. **失败标记**: 如果即时抽取失败，则将状态更新为 `pending_review`，交由人工审核。
      这一设计将学习成果立刻转化为生产力，实现了“学习-应用”的无缝衔接。

### 3.5. 任务 #16: 实施第四阶段 - 开发批量抽取工作流

- **目标**: 将高质量的已知事件转化为最终的结构化知识。
- **实施要点**:
  - **创建 `run_extraction_workflow.py`**:
    - 从数据库查询状态为 `pending_extraction` 的数据，按批次处理。
    - 调用 `ExtractionAgent` 进行信息抽取。
    - 将最终的结构化数据保存到指定的输出位置（如另一个数据表或jsonl文件）。
    - 调用 `database_manager` 将已处理数据的状态更新为 `completed`。

### 3.6. 补充建议：统一的管理入口

- **目标**: 提升系统的整体性和易用性。
- **实施建议**:
  - 利用 `src/admin/admin_module.py` 创建一个统一的命令行入口。
  - 例如: `python -m src.admin.cli --action triage` 或 `python -m src.admin.cli --action learn`。
  - 这将使系统更像一个专业的应用程序，而非一堆独立的脚本。

---
*本文档是V3.1开发与实施的唯一、最终的指导方针。*

---

## 4. V4.0 架构演进：知识增强的闭环智能体协作

V3.1为我们构建了一个鲁棒的数据处理流水线。V4.0的目标是在此基础上，通过激活双数据库（图数据库与向量数据库）并引入知识反馈循环，将系统从一个“数据处理工厂”升级为一个“知识推理引擎”。

### 4.1 核心理念

当前的V3.1是**单向流水线**，而V4.0将是一个**闭环系统**。系统的每一次运行不仅会产出新知识，更会用这些新知识来**增强**后续的处理过程，让智能体在每一次处理中都变得更“博学”。

### 4.2 架构图 (V4.0)

```mermaid
graph TD
    subgraph V3.1: 数据处理流水线 (已实现)
        A[原始文本] --> B(run_batch_triage.py);
        B --> C{人工审核};
        C --> D(run_learning_workflow.py);
        C --> E(run_extraction_workflow.py);
        E --> F[扁平的事件JSONL];
    end

    subgraph V4.0: 关系分析与知识存储层 (待补充)
        F --> G(run_relationship_analysis.py);
        G --> H{事件节点 + 关系边};
        H --> I(GraphStorageAgent);
        H --> J(VectorStorageAgent);
        I --> K[图数据库 Neo4j];
        J --> L[向量数据库 ChromaDB];
    end

    subgraph V4.0: 知识反馈与增强回路 (待补充)
        K & L --> M(HybridRetrieverAgent);
        M --> G;
    end
```

### 4.3 缺失功能与实现细节

#### 4.3.1 Task #18: 实现关系分析与知识存储层

此任务的目标是将离散的事件列表，转化为一个相互关联的、可查询的知识网络。

-   **关系分析 (`RelationshipAnalysisToolkit`)**:
    -   **输入**: 源自同一篇原文的所有事件。
    -   **核心逻辑**: 使用LLM分析事件组之间的深层逻辑关系。
    -   **关系类型体系**:
        | 关系类型 | 定义 |
        | :--- | :--- |
        | `Causal` | 事件A是事件B发生的原因或前提。 |
        | `Temporal` | 事件A明确发生在事件B之前或之后。 |
        | `Sub-event` | 事件A是构成更宏观的事件B的一个具体组成部分。 |
        | `Elaboration` | 事件A是对事件B的进一步详细说明、解释或举例。 |
        | `Contradiction` | 事件A与事件B在事实上相互矛盾或对立。 |
        | `Influence` | 事件A可能对事件B产生影响，但因果性不强。 |
        | `Related` | 事件A和事件B在主题或实体上相关，但无法归��以上强逻辑关系。 |

-   **图数据库存储 (`GraphStorageAgent`)**:
    -   **职责**: 构建精确的、结构化的知识网络。
    -   **实现**: 将事件和实体创建为**节点**，将分析出的关系和实体参与关系创建为**边**，并存入图数据库。

-   **向量数据库存储 (`VectorStorageAgent`)**:
    -   **职责**: 构建可进行模糊语义搜索的记忆库。
    -   **实现 (分层向量化策略)**:
        1.  **原文片段 (`_source_text`)**: 用于宏观主题搜索。
        2.  **事件描述 (`event.description`)**: 用于精确的原子事件搜索。
        3.  **实体中心上下文 (`entity_centric_context`)**: 拼接“实体+事件类型+描述”，用于回答“某实体干了什么”的问题。

#### 4.3.2 Task #19: 开发混合检索器并闭合知识循环

此任务是实现系统“智能”的关键，让系统能够利用已有的知识来更好地理解新信息。

-   **核心Agent (`HybridRetrieverAgent`)**:
    -   **职责**: 充当“情报分析师”，在处理新信息前，从双数据库中检索相关的历史背景。
    -   **触发时机**: 优先在**关系分析阶段**引入。

-   **混合检索策略**:
    1.  对新文本进行快速实体识别。
    2.  **并行查询**:
        -   **图数据库**: 基于核心实体进行精确的、结构化的事实查询。
        -   **向量数据库**: 基于全文向量进行模糊的、语义相似的案例查询。

-   **知识闭环 (Prompt增强)**:
    -   将检索到的图谱事实和相似案例整合成一段“背景摘要”。
    -   将此摘要注入到关系分析任务的Prompt中，为LLM提供决策参考，实现知识增强。

### 4.4 并行开发策略 (Task #20)

为了最大化开发效率，项目采用并行处理策略。在V4.0的核心功能（关系分析、知识图谱构建、混合检索）进行开发的同时，将**并行地**对全量原始数据执行初步的事件抽取（Task #16的逻辑）。

-   **目标**: 提前完成最耗时的数据预处理步骤。
-   **产出**: 一个临时的、包含所有原子事件的 `structured_events.jsonl` 文件。
-   **价值**: 这个文件将作为V4.0功能开发完成后的**直接输入**，极大地缩短了从开发完成到产出最终知识图谱的等待时间。同时，它也为我们提供了一个早期的数据集，用于评估抽取Prompt的泛化能力。

---

### 4.5 V4.0 架构演进的核心：Cortex 智能上下文引擎 (Task #21)

V4.0 的关系分析工作流 (Task #18) 必须处理来源模糊、高度离散的事件数据。为了解决这一核心挑战，我们设计了 **Cortex** 模块。

#### 4.5.1 设计初衷 (The Why)

关系分析只有在有意义的上下文中才能产生价值。Cortex 的核心使命，就是在海量、零散的事件中**重建上下文**，将描述同一个“故事”的事件片段智能地聚合在一起，为下游的关系��析提供高质量的、逻辑内聚的输入。它将系统从处理“单点信息”提升到理解“故事脉络”的层面。

#### 4.5.2 架构与组件 (The What)

Cortex 是一个独立的、上游的预处理模块，由一个专用的工作流脚本 `run_cortex_workflow.py` 驱动。它包含三个核心组件：

1.  **`VectorizationService` (向量化服务)**: 负责将所有事件描述实时地转换为向量，并存储于ChromaDB，为所有需要语义理解的模块提供基础。
2.  **`ClusteringOrchestrator` (聚类协调器)**: 负责执行算法层面的“粗聚类”。它从向量数据库中提取事件向量，结合实体信息进行加权，使用DBSCAN等算法形成初步的“粗簇”，并将聚类结果（`event_id` -> `cluster_id`）写回中央状态库。
3.  **`RefinementAgent` (精炼智能体)**: 负责对“粗簇”进行LLM驱动的“精细化”，将其拆分为一个或多个强相关的“故事单元”(Story)。

#### 4.5.3 执行流程与交互 (The How)

Cortex 的工作流由 `run_cortex_workflow.py` 驱动，其内部流程如下：

1.  **算法粗聚类**: `ClusteringOrchestrator` 首先运行，对所有 `pending_clustering` 状态的事件进行向量化和聚类，形成“粗簇”。
2.  **LLM精炼**:
    -   工作流遍历所有新生成的“粗簇”。
    -   `RefinementAgent` 被调用，对每个“粗簇”进行分析。
    -   **大簇处理策略**: 如果一个簇的规模过大（例如 > 25个事件），Agent将采用 **“摘要-检索-扩展”** 策略：
        1.  随机抽样N个事件，让LLM**总结**出核心主题。
        2.  将该主题作为Query，在簇内进行向量**检索**，找到最相关的K个事件，形成第一个“故事单元”。
        3.  将已处理的事件移出，对剩余事件重复此过程，直到整个大簇被分解完毕。
    -   LLM返回一个或多个“故事单元”（`Story`）。
3.  **状态更新**: 系统为每个 `Story` 生成唯一的 `story_id`，并更新其中所有事件的状态为 `pending_relationship_analysis`，等待下游处理。

```mermaid
graph TD
    subgraph Cortex Workflow (run_cortex_workflow.py)
        A[开始] --> B(ClusteringOrchestrator: 算法粗聚类);
        B --> C{For 循环: 遍历每个粗簇};
        C --> D{簇是否过大?};
        D -->|否| E(RefinementAgent: 直接精炼);
        D -->|是| F(RefinementAgent: 执行“摘要-检索-扩展”策略);
        E --> G(生成精细“故事单元”);
        F --> G;
        G --> H(更新数据库状态 -> pending_relationship_analysis);
        H --> C;
        C -->|循环结束| I[结束];
    end
```

#### 4.5.4 调用方式：批处理触发

Cortex 模块与下游的关系分析模块是**异步解耦**的，通过数据库状态进行“接力”。其自身的调用，则采用高效的 **“批处理触发”** 模式：

-   **触发点**: 位于上游的 `run_extraction_workflow.py` (Task #16) 的执行末尾。
-   **逻辑**:
    1.  当事件抽取工作流完成一批新事件的抽取和入库后，它会检查当前数据库中状态为 `pending_clustering` 的事件总数。
    2.  **如果总数达到预设阈值（例如100）**，该脚本就会以子进程的方式，自动触发 `run_cortex_workflow.py` 的执行。
    3.  如果未达到阈值，则本次不触发，等待下一次抽取任务带来更多新事件。

这种设计兼顾了资源效率和准实时性，确保了系统流水线的流畅运转，是当前阶段的最佳实现方案。

