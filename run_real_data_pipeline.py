#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®æ•°æ®æµæ°´çº¿è¿è¡Œè„šæœ¬
ä½¿ç”¨ IC_data/filtered_data_demo.json ä¸­çš„çœŸå®åŠå¯¼ä½“è¡Œä¸šæ–°é—»æ•°æ®
æµ‹è¯•å®Œæ•´çš„äº‹ç†å›¾è°±æ„å»ºæµç¨‹
"""

import sys
import os
import json
import asyncio
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from src.event_extraction.deepseek_extractor import DeepSeekEventExtractor
from src.event_logic.event_logic_analyzer import EventLogicAnalyzer
from src.event_logic.hybrid_retriever import HybridRetriever
from src.event_logic.attribute_enhancer import AttributeEnhancer
from src.event_logic.pattern_discoverer import PatternDiscoverer
from src.output.jsonl_manager import JSONLManager
from src.output.graph_exporter import GraphExporter
from src.core.workflow_controller import WorkflowController
from src.config.workflow_config import ConfigManager
from src.models.event_data_model import Event, EventRelation
from src.monitoring.performance_monitor import PerformanceMonitor

class RealDataPipeline:
    """çœŸå®æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, config_path: str = "config/settings.yaml"):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        self.config_manager = ConfigManager(config_path)
        self.performance_monitor = PerformanceMonitor()
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.event_extractor = DeepSeekEventExtractor()
        self.logic_analyzer = EventLogicAnalyzer()
        self.hybrid_retriever = HybridRetriever()
        self.attribute_enhancer = AttributeEnhancer()
        self.pattern_discoverer = PatternDiscoverer()
        self.jsonl_manager = JSONLManager()
        self.graph_exporter = GraphExporter()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("output/real_data_results")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"âœ… æµæ°´çº¿åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_real_data(self, data_path: str) -> list:
        """åŠ è½½çœŸå®æ•°æ®"""
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡çœŸå®æ–°é—»æ•°æ®")
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def extract_events_from_texts(self, texts: list) -> list:
        """ä»æ–‡æœ¬ä¸­æŠ½å–äº‹ä»¶"""
        print("\nğŸ”„ å¼€å§‹äº‹ä»¶æŠ½å–...")
        all_events = []
        
        for i, text in enumerate(texts, 1):
            try:
                print(f"å¤„ç†ç¬¬ {i}/{len(texts)} æ¡æ–°é—»...")
                
                # ä½¿ç”¨DeepSeekè¿›è¡Œäº‹ä»¶æŠ½å–
                extracted_data = await self.event_extractor.extract_events(text)
                
                if extracted_data and 'events' in extracted_data:
                    events = extracted_data['events']
                    print(f"  âœ… æŠ½å–åˆ° {len(events)} ä¸ªäº‹ä»¶")
                    
                    # è½¬æ¢ä¸ºEventå¯¹è±¡
                    for event_data in events:
                        event = Event(
                            id=f"evt_{i}_{len(all_events)+1}",
                            summary=event_data.get('summary', ''),
                            text=text,
                            event_type=event_data.get('event_type', 'unknown'),
                            timestamp=datetime.now(),
                            participants=event_data.get('participants', []),
                            properties=event_data
                        )
                        all_events.append(event)
                else:
                    print(f"  âš ï¸ æœªèƒ½æŠ½å–åˆ°æœ‰æ•ˆäº‹ä»¶")
                    
            except Exception as e:
                print(f"  âŒ å¤„ç†ç¬¬ {i} æ¡æ–°é—»æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"\nâœ… äº‹ä»¶æŠ½å–å®Œæˆï¼Œå…±æŠ½å– {len(all_events)} ä¸ªäº‹ä»¶")
        return all_events
    
    def analyze_event_relations(self, events: list) -> list:
        """åˆ†æäº‹ä»¶å…³ç³»"""
        print("\nğŸ”„ å¼€å§‹äº‹ç†å…³ç³»åˆ†æ...")
        
        try:
            relations = self.logic_analyzer.analyze_event_relations(events)
            print(f"âœ… å…³ç³»åˆ†æå®Œæˆï¼Œå‘ç° {len(relations)} ä¸ªå…³ç³»")
            return relations
        except Exception as e:
            print(f"âŒ å…³ç³»åˆ†æå¤±è´¥: {e}")
            return []
    
    def enhance_with_graphrag(self, events: list, relations: list):
        """ä½¿ç”¨GraphRAGå¢å¼º"""
        print("\nğŸ”„ å¼€å§‹GraphRAGå¢å¼º...")
        
        try:
            # å±æ€§è¡¥å……
            enhanced_events = []
            for event in events:
                enhanced_event = self.attribute_enhancer.enhance_event_attributes(event)
                enhanced_events.append(enhanced_event)
            
            # æ¨¡å¼å‘ç°
            patterns = self.pattern_discoverer.discover_patterns(enhanced_events)
            
            print(f"âœ… GraphRAGå¢å¼ºå®Œæˆ")
            print(f"  - å¢å¼ºäº‹ä»¶: {len(enhanced_events)} ä¸ª")
            print(f"  - å‘ç°æ¨¡å¼: {len(patterns)} ä¸ª")
            
            return enhanced_events, patterns
            
        except Exception as e:
            print(f"âŒ GraphRAGå¢å¼ºå¤±è´¥: {e}")
            return events, []
    
    def export_results(self, events: list, relations: list, patterns: list):
        """å¯¼å‡ºç»“æœ"""
        print("\nğŸ”„ å¼€å§‹å¯¼å‡ºç»“æœ...")
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # å¯¼å‡ºJSONLæ ¼å¼
            events_file = self.output_dir / f"events_{timestamp}.jsonl"
            relations_file = self.output_dir / f"relations_{timestamp}.jsonl"
            combined_file = self.output_dir / f"combined_{timestamp}.jsonl"
            
            self.jsonl_manager.export_events(events, str(events_file))
            self.jsonl_manager.export_relations(relations, str(relations_file))
            self.jsonl_manager.export_combined_data(events, relations, str(combined_file))
            
            # å¯¼å‡ºå›¾è°±æ ¼å¼
            graph_file = self.output_dir / f"graph_{timestamp}.graphml"
            self.graph_exporter.export_to_graphml(events, relations, str(graph_file))
            
            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            report_file = self.output_dir / f"report_{timestamp}.json"
            report = {
                "timestamp": timestamp,
                "statistics": {
                    "total_events": len(events),
                    "total_relations": len(relations),
                    "total_patterns": len(patterns),
                    "event_types": list(set(event.event_type for event in events)),
                    "relation_types": list(set(rel.relation_type.value for rel in relations))
                },
                "files": {
                    "events": str(events_file),
                    "relations": str(relations_file),
                    "combined": str(combined_file),
                    "graph": str(graph_file)
                }
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ç»“æœå¯¼å‡ºå®Œæˆ:")
            print(f"  - äº‹ä»¶æ–‡ä»¶: {events_file}")
            print(f"  - å…³ç³»æ–‡ä»¶: {relations_file}")
            print(f"  - åˆå¹¶æ–‡ä»¶: {combined_file}")
            print(f"  - å›¾è°±æ–‡ä»¶: {graph_file}")
            print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {report_file}")
            
        except Exception as e:
            print(f"âŒ ç»“æœå¯¼å‡ºå¤±è´¥: {e}")
    
    async def run_pipeline(self, data_path: str):
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        print("ğŸš€ å¼€å§‹è¿è¡ŒçœŸå®æ•°æ®å¤„ç†æµæ°´çº¿")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_path}")
        print("=" * 60)
        
        # å¼€å§‹æ€§èƒ½ç›‘æ§
        self.performance_monitor.start_monitoring()
        
        try:
            # 1. åŠ è½½çœŸå®æ•°æ®
            texts = self.load_real_data(data_path)
            if not texts:
                print("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return
            
            # 2. äº‹ä»¶æŠ½å–
            events = await self.extract_events_from_texts(texts)
            if not events:
                print("âŒ æœªèƒ½æŠ½å–åˆ°äº‹ä»¶ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return
            
            # 3. å…³ç³»åˆ†æ
            relations = self.analyze_event_relations(events)
            
            # 4. GraphRAGå¢å¼º
            enhanced_events, patterns = self.enhance_with_graphrag(events, relations)
            
            # 5. å¯¼å‡ºç»“æœ
            self.export_results(enhanced_events, relations, patterns)
            
            # 6. æ€§èƒ½ç»Ÿè®¡
            performance_stats = self.performance_monitor.get_performance_stats()
            print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"  - æ€»å¤„ç†æ—¶é—´: {performance_stats.get('total_time', 0):.2f}s")
            print(f"  - å†…å­˜ä½¿ç”¨: {performance_stats.get('memory_usage', 0):.2f}MB")
            
            print("\nğŸ‰ æµæ°´çº¿è¿è¡Œå®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ æµæ°´çº¿è¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            self.performance_monitor.stop_monitoring()

def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_path = "IC_data/filtered_data_demo.json"
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return
    
    # åˆ›å»ºå¹¶è¿è¡Œæµæ°´çº¿
    pipeline = RealDataPipeline()
    
    # è¿è¡Œå¼‚æ­¥æµæ°´çº¿
    asyncio.run(pipeline.run_pipeline(data_path))

if __name__ == "__main__":
    main()