#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®æ•°æ®æµæ°´çº¿è¿è¡Œè„šæœ¬
ä½¿ç”¨ IC_data/filtered_data_demo.json ä¸­çš„çœŸå®åŠå¯¼ä½“è¡Œä¸šæ–°é—»æ•°æ®
æµ‹è¯•å®Œæ•´çš„äº‹ç†å›¾è°±æ„å»ºæµç¨‹
"""

import sys
import os
import json
import asyncio
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from src.event_extraction.deepseek_extractor import DeepSeekEventExtractor
from src.event_logic.event_logic_analyzer import EventLogicAnalyzer
from src.event_logic.hybrid_retriever import HybridRetriever
from src.event_logic.attribute_enhancer import AttributeEnhancer, IncompleteEvent
from src.event_logic.pattern_discoverer import PatternDiscoverer
from src.output.jsonl_manager import JSONLManager
from src.output.graph_exporter import GraphExporter
from src.core.workflow_controller import WorkflowController
from src.config.workflow_config import ConfigManager
from src.models.event_data_model import Event, EventRelation
from src.monitoring.performance_monitor import PerformanceMonitor

class RealDataPipeline:
    """çœŸå®æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, config_dir: str = "config"):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        self.config_manager = ConfigManager(config_dir)
        self.performance_monitor = PerformanceMonitor()
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.event_extractor = DeepSeekEventExtractor()
        # æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼Œè®¾ç½®æ¨¡å‹åç§°
        self.event_extractor.model_name = "deepseek-reasoner"
        
        self.logic_analyzer = EventLogicAnalyzer()
        self.hybrid_retriever = HybridRetriever()
        self.attribute_enhancer = AttributeEnhancer(self.hybrid_retriever)
        self.pattern_discoverer = PatternDiscoverer(self.hybrid_retriever)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("output/real_data_results")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨æ­£ç¡®çš„è¾“å‡ºç›®å½•åˆå§‹åŒ–ç®¡ç†å™¨
        self.jsonl_manager = JSONLManager(output_dir=str(self.output_dir))
        self.graph_exporter = GraphExporter(output_dir=str(self.output_dir))
        
        print(f"âœ… æµæ°´çº¿åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_real_data(self, data_path: str) -> list:
        """åŠ è½½çœŸå®æ•°æ®å¹¶ä»…è¿”å›æ–‡æœ¬å†…å®¹"""
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡çœŸå®æ–°é—»æ•°æ®")
            # ç¡®ä¿è¿”å›çš„æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬å­—ç¬¦ä¸²åˆ—è¡¨
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def extract_events_from_texts(self, texts: list) -> list:
        """ä»æ–‡æœ¬åˆ—è¡¨ä¸­æŠ½å–äº‹ä»¶"""
        print("\nğŸ”„ å¼€å§‹äº‹ä»¶æŠ½å–...")
        all_events = []
        
        for i, text_content in enumerate(texts, 1):
            try:
                print(f"å¤„ç†ç¬¬ {i}/{len(texts)} æ¡æ–°é—»...")
                
                if not isinstance(text_content, str):
                    print(f"  âš ï¸ ç¬¬ {i} æ¡æ•°æ®ä¸æ˜¯æœ‰æ•ˆæ–‡æœ¬ï¼Œå·²è·³è¿‡ã€‚")
                    continue

                extracted_events_data = await self.event_extractor.extract_multi_events(text_content)
                
                if extracted_events_data:
                    print(f"  âœ… æŠ½å–åˆ° {len(extracted_events_data)} ä¸ªäº‹ä»¶")
                    
                    for event_data in extracted_events_data:
                        if not isinstance(event_data, dict):
                            print(f"  âš ï¸ æ— æ•ˆçš„äº‹ä»¶æ•°æ®æ ¼å¼ï¼Œå·²è·³è¿‡: {event_data}")
                            continue
                        event = Event(
                            id=f"evt_{i}_{len(all_events)+1}",
                            summary=event_data.get('summary', ''),
                            text=text_content,
                            event_type=event_data.get('event_type', 'unknown'),
                            timestamp=datetime.now(),
                            participants=event_data.get('participants', []),
                            properties=event_data
                        )
                        all_events.append(event)
                else:
                    print(f"  âš ï¸ æœªèƒ½æŠ½å–åˆ°æœ‰æ•ˆäº‹ä»¶")
                    
            except Exception as e:
                print(f"  âŒ å¤„ç†ç¬¬ {i} æ¡æ–°é—»æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"\nâœ… äº‹ä»¶æŠ½å–å®Œæˆï¼Œå…±æŠ½å– {len(all_events)} ä¸ªäº‹ä»¶")
        return all_events
    
    def analyze_event_relations(self, events: list) -> list:
        """åˆ†æäº‹ä»¶å…³ç³»"""
        print("\nğŸ”„ å¼€å§‹äº‹ç†å…³ç³»åˆ†æ...")
        
        try:
            relations = self.logic_analyzer.analyze_event_relations(events)
            print(f"âœ… å…³ç³»åˆ†æå®Œæˆï¼Œå‘ç° {len(relations)} ä¸ªå…³ç³»")
            return relations
        except Exception as e:
            print(f"âŒ å…³ç³»åˆ†æå¤±è´¥: {e}")
            return []
    
    def enhance_with_graphrag(self, events: list, relations: list):
        """ä½¿ç”¨GraphRAGå¢å¼º"""
        print("\nğŸ”„ å¼€å§‹GraphRAGå¢å¼º...")
        
        try:
            enhanced_events_data = []
            for event in events:
                missing_attrs = self.attribute_enhancer.supported_attributes
                incomplete_event = IncompleteEvent(
                    id=event.id,
                    description=event.summary or event.text,
                    timestamp=event.timestamp,
                    event_type=event.event_type,
                    participants=event.participants,
                    missing_attributes=missing_attrs
                )
                enhanced_event_data = self.attribute_enhancer.enhance_event(incomplete_event)
                enhanced_events_data.append(enhanced_event_data)

            final_enhanced_events = []
            for enhanced_data in enhanced_events_data:
                original_event = next((e for e in events if e.id == enhanced_data.original_event.id), None)
                if not original_event:
                    continue
                new_properties = original_event.properties.copy()
                new_properties.update(enhanced_data.enhanced_attributes)
                enhanced_event = Event(
                    id=original_event.id,
                    summary=original_event.summary,
                    text=original_event.text,
                    event_type=new_properties.get('event_type', original_event.event_type),
                    timestamp=new_properties.get('timestamp', original_event.timestamp),
                    participants=new_properties.get('participants', original_event.participants),
                    properties=new_properties
                )
                final_enhanced_events.append(enhanced_event)

            print(f"  - å±æ€§è¡¥å……å®Œæˆï¼Œç°åœ¨å¼€å§‹æ¨¡å¼å‘ç°...")
            patterns = self.pattern_discoverer.discover_patterns(final_enhanced_events)
            
            print(f"âœ… GraphRAGå¢å¼ºå®Œæˆ")
            print(f"  - å¢å¼ºäº‹ä»¶: {len(final_enhanced_events)} ä¸ª")
            print(f"  - å‘ç°æ¨¡å¼: {len(patterns)} ä¸ª")
            
            return final_enhanced_events, patterns
            
        except Exception as e:
            print(f"âŒ GraphRAGå¢å¼ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return events, []
    
    def export_results(self, events: list, relations: list, patterns: list):
        """å¯¼å‡ºç»“æœ"""
        print("\nğŸ”„ å¼€å§‹å¯¼å‡ºç»“æœ...")
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # åªä¼ é€’æ–‡ä»¶åç»™ç®¡ç†å™¨
            events_filename = f"events_{timestamp}.jsonl"
            relations_filename = f"relations_{timestamp}.jsonl"
            combined_filename = f"combined_{timestamp}.jsonl"
            graph_filename = f"graph_{timestamp}.graphml"
            report_filename = f"report_{timestamp}.json"

            self.jsonl_manager.write_events_to_jsonl(events, events_filename)
            self.jsonl_manager.write_relations_to_jsonl(relations, relations_filename)
            self.jsonl_manager.write_combined_to_jsonl(events, relations, combined_filename)
            self.graph_exporter.export_to_graphml(events, relations, graph_filename)
            
            # ä¸ºæŠ¥å‘Šæ„å»ºå®Œæ•´è·¯å¾„
            full_report_path = self.output_dir / report_filename
            report = {
                "timestamp": timestamp,
                "statistics": {
                    "total_events": len(events),
                    "total_relations": len(relations),
                    "total_patterns": len(patterns),
                    "event_types": list(set(event.event_type for event in events)),
                    "relation_types": list(set(rel.relation_type.value for rel in relations) if relations else [])
                },
                "files": {
                    "events": str(self.output_dir / events_filename),
                    "relations": str(self.output_dir / relations_filename),
                    "combined": str(self.output_dir / combined_filename),
                    "graph": str(self.output_dir / graph_filename)
                }
            }
            
            with open(full_report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ç»“æœå¯¼å‡ºå®Œæˆ:")
            print(f"  - äº‹ä»¶æ–‡ä»¶: {self.output_dir / events_filename}")
            print(f"  - å…³ç³»æ–‡ä»¶: {self.output_dir / relations_filename}")
            print(f"  - åˆå¹¶æ–‡ä»¶: {self.output_dir / combined_filename}")
            print(f"  - å›¾è°±æ–‡ä»¶: {self.output_dir / graph_filename}")
            print(f"  - ç»Ÿè®¡æŠ¥å‘Š: {full_report_path}")
            
        except Exception as e:
            print(f"âŒ ç»“æœå¯¼å‡ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    async def run_pipeline(self, data_path: str):
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        print("ğŸš€ å¼€å§‹è¿è¡ŒçœŸå®æ•°æ®å¤„ç†æµæ°´çº¿")
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_path}")
        print("=" * 60)
        
        self.performance_monitor.start()
        
        try:
            texts = self.load_real_data(data_path)
            if not texts:
                print("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return
            
            events = await self.extract_events_from_texts(texts)
            if not events:
                print("âŒ æœªèƒ½æŠ½å–åˆ°äº‹ä»¶ï¼Œæµæ°´çº¿ç»ˆæ­¢")
                return
            
            relations = self.analyze_event_relations(events)
            
            enhanced_events, patterns = self.enhance_with_graphrag(events, relations)
            
            self.export_results(enhanced_events, relations, patterns)
            
            performance_stats = self.performance_monitor.get_performance_summary()
            print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            if performance_stats:
                print(json.dumps(performance_stats, indent=2, default=str))
            
            print("\nğŸ‰ æµæ°´çº¿è¿è¡Œå®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ æµæ°´çº¿è¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            self.performance_monitor.stop()

def main():
    """ä¸»å‡½æ•°"""
    data_path = "IC_data/filtered_data_demo.json"
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return
    
    pipeline = RealDataPipeline()
    
    asyncio.run(pipeline.run_pipeline(data_path))

if __name__ == "__main__":
    main()

