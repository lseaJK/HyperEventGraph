#!/usr/bin/env python3
"""
Enhanced Cortex Agent - æ™ºèƒ½å¤šç»´åº¦èšç±»å®ç°
ä½œè€…: HyperEventGraph æ¶æ„å¸ˆ
æ—¶é—´: 2025-08-16

æ ¸å¿ƒæ”¹è¿›ï¼š
1. å¤šç»´åº¦ç‰¹å¾æå–ï¼ˆæ—¶é—´ã€å®ä½“ã€è¯­ä¹‰ã€ç±»å‹ï¼‰
2. å±‚æ¬¡åŒ–èšç±»ç­–ç•¥
3. åŠ¨æ€å‚æ•°è°ƒæ•´
4. æ™ºèƒ½æ•…äº‹ç”Ÿæˆ
"""

import json
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
import re
from pathlib import Path
import sys

# Add project root to sys.path
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.config_loader import get_config
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedCortexAgent:
    """å¢å¼ºç‰ˆ Cortex ä»£ç† - æ™ºèƒ½äº‹ä»¶èšç±»å’Œæ•…äº‹ç”Ÿæˆ"""
    
    def __init__(self, embedding_model=None):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆ Cortex ä»£ç†"""
        self.config = get_config()
        self.cortex_config = self.config.get('cortex', {})
        
        # èšç±»å‚æ•°
        self.time_window_days = self.cortex_config.get('time_window_days', 30)
        self.entity_weight = self.cortex_config.get('entity_weight', 0.3)
        self.semantic_weight = self.cortex_config.get('semantic_weight', 0.4)
        self.time_weight = self.cortex_config.get('time_weight', 0.2)
        self.type_weight = self.cortex_config.get('type_weight', 0.1)
        
        # DBSCAN å‚æ•°
        self.dbscan_eps = self.cortex_config.get('dbscan_eps', 0.6)
        self.dbscan_min_samples = self.cortex_config.get('dbscan_min_samples', 2)
        
        # åŠ è½½åµŒå…¥æ¨¡å‹
        if embedding_model:
            self.embedding_model = embedding_model
        else:
            self._load_embedding_model()
    
    def _load_embedding_model(self):
        """åŠ è½½ BGE åµŒå…¥æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            model_name = self.cortex_config.get('vectorizer', {}).get('model_name', 'BAAI/bge-large-zh-v1.5')
            # ä½¿ç”¨é…ç½®ä¸­çš„ç¼“å­˜ç›®å½•
            cache_dir = self.config.get('model_settings', {}).get('cache_dir', '/home/kai/models')
            self.embedding_model = SentenceTransformer(model_name, cache_folder=cache_dir)
            logger.info(f"âœ… åŠ è½½åµŒå…¥æ¨¡å‹: {model_name} (ç¼“å­˜: {cache_dir})")
        except Exception as e:
            logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.embedding_model = None
    
    def parse_event_date(self, date_str: str) -> datetime:
        """è§£æå¤šç§æ ¼å¼çš„äº‹ä»¶æ—¥æœŸï¼Œå°½é‡ä¿æŒåŸå§‹ç²¾åº¦"""
        if not date_str:
            return datetime(2023, 1, 1)  # é»˜è®¤å‡è®¾2023å¹´å¼€å§‹
        
        date_str = str(date_str).strip()
        
        # å¸¸è§æ—¥æœŸæ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ŒæŒ‰ç²¾åº¦ä»é«˜åˆ°ä½æ’åº
        patterns = [
            # å®Œæ•´æ—¥æœŸ
            (r'(\d{4})-(\d{1,2})-(\d{1,2})', lambda m: datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))),
            # å¹´-æœˆ
            (r'(\d{4})-(\d{1,2})$', lambda m: datetime(int(m.group(1)), int(m.group(2)), 1)),  # æœˆåˆ
            # åŠå¹´æ ¼å¼
            (r'(\d{4})-H(\d)', lambda m: datetime(int(m.group(1)), 6 if int(m.group(2)) == 2 else 1, 1)),
            # å­£åº¦æ ¼å¼  
            (r'(\d{4})-Q(\d)', lambda m: datetime(int(m.group(1)), (int(m.group(2)) - 1) * 3 + 1, 1)),
            # å¹´ä»½
            (r'(\d{4})$', lambda m: datetime(int(m.group(1)), 1, 1)),  # å¹´åˆ
        ]
        
        # å°è¯•ç²¾ç¡®åŒ¹é…
        for pattern, converter in patterns:
            match = re.match(pattern, date_str)
            if match:
                try:
                    return converter(match)
                except (ValueError, IndexError):
                    continue
        
        # å¯¹äºå¤æ‚æ ¼å¼ï¼Œæå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„å¹´ä»½ï¼Œä¿æŒå¹´ä»½ç²¾åº¦
        year_match = re.search(r'(\d{4})', date_str)
        if year_match:
            year = int(year_match.group(1))
            # æ£€æŸ¥æ˜¯å¦æœ‰æœˆä»½ä¿¡æ¯
            month_match = re.search(r'-(\d{1,2})', date_str)
            if month_match:
                try:
                    month = int(month_match.group(1))
                    if 1 <= month <= 12:
                        return datetime(year, month, 1)
                except ValueError:
                    pass
            # åªæœ‰å¹´ä»½ä¿¡æ¯
            return datetime(year, 1, 1)
        
        # å®Œå…¨æ— æ³•è§£ææ—¶çš„å¤‡é€‰æ–¹æ¡ˆ
        logger.warning(f"âš ï¸ æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}ï¼Œä¿ç•™åŸå§‹å­—ç¬¦ä¸²ï¼Œä½¿ç”¨2023-01-01ä½œä¸ºæ’åºåŸºå‡†")
        return datetime(2023, 1, 1)
    
    def extract_event_features(self, events: List[Dict]) -> np.ndarray:
        """æå–å¤šç»´åº¦äº‹ä»¶ç‰¹å¾"""
        logger.info(f"ğŸ” å¼€å§‹æå– {len(events)} ä¸ªäº‹ä»¶çš„ç‰¹å¾...")
        
        features = []
        
        for i, event in enumerate(events):
            feature_vector = []
            
            # 1. æ—¶é—´ç‰¹å¾
            event_date = self.parse_event_date(event.get('event_date', ''))
            timestamp = event_date.timestamp()
            feature_vector.append(timestamp)
            
            # 2. å®ä½“ç‰¹å¾ - è®¡ç®—å®ä½“æ•°é‡å’Œé‡è¦æ€§
            entities = event.get('involved_entities', [])
            if isinstance(entities, str):
                try:
                    entities = json.loads(entities)
                except:
                    entities = []
            
            entity_count = len(entities) if entities else 0
            entity_types = set()
            entity_roles = set()
            
            for entity in entities:
                if isinstance(entity, dict):
                    entity_types.add(entity.get('entity_type', ''))
                    entity_roles.add(entity.get('role_in_event', ''))
            
            feature_vector.extend([
                entity_count,
                len(entity_types),
                len(entity_roles)
            ])
            
            # 3. äº‹ä»¶ç±»å‹ç‰¹å¾
            event_type = event.get('assigned_event_type', event.get('event_type', ''))
            micro_type = event.get('micro_event_type', '')
            
            # ç®€å•çš„ç±»å‹ç¼–ç ï¼ˆå¯ä»¥ç”¨æ›´å¤æ‚çš„åµŒå…¥ï¼‰
            type_hash = hash(event_type) % 100
            micro_hash = hash(micro_type) % 100
            
            feature_vector.extend([type_hash, micro_hash])
            
            # 4. é‡åŒ–æ•°æ®ç‰¹å¾
            structured_data = event.get('structured_data', {})
            if isinstance(structured_data, str):
                try:
                    structured_data = json.loads(structured_data)
                except:
                    structured_data = {}
            
            quantitative_data = structured_data.get('quantitative_data', {})
            has_quantitative = 1 if quantitative_data else 0
            
            feature_vector.append(has_quantitative)
            
            features.append(feature_vector)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        features_array = np.array(features)
        normalized_features = scaler.fit_transform(features_array)
        
        logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆ: {normalized_features.shape}")
        return normalized_features
    
    def extract_semantic_features(self, events: List[Dict]) -> np.ndarray:
        """æå–è¯­ä¹‰ç‰¹å¾ï¼ˆä½¿ç”¨ BGE æ¨¡å‹ï¼‰"""
        if not self.embedding_model:
            logger.warning("âš ï¸ åµŒå…¥æ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡è¯­ä¹‰ç‰¹å¾")
            return np.zeros((len(events), 1024))  # BGE æ¨¡å‹çš„ç»´åº¦
        
        logger.info("ğŸ” æå–è¯­ä¹‰ç‰¹å¾...")
        
        texts = []
        for event in events:
            # ç»„åˆå¤šä¸ªæ–‡æœ¬å­—æ®µ
            description = ""
            structured_data = event.get('structured_data', {})
            if isinstance(structured_data, str):
                try:
                    structured_data = json.loads(structured_data)
                except:
                    structured_data = {}
            
            if structured_data:
                description = structured_data.get('description', '')
            
            if not description:
                description = event.get('source_text', '')[:200]  # æˆªå–å‰200å­—ç¬¦
            
            texts.append(description or "æ— æè¿°")
        
        # æ‰¹é‡ç¼–ç 
        try:
            embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
            logger.info(f"âœ… è¯­ä¹‰ç‰¹å¾æå–å®Œæˆ: {embeddings.shape}")
            return embeddings
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros((len(events), 1024))
    
    def calculate_entity_similarity_matrix(self, events: List[Dict]) -> np.ndarray:
        """è®¡ç®—å®ä½“ç›¸ä¼¼æ€§çŸ©é˜µ"""
        n_events = len(events)
        similarity_matrix = np.zeros((n_events, n_events))
        
        for i in range(n_events):
            for j in range(i + 1, n_events):
                # è·å–ä¸¤ä¸ªäº‹ä»¶çš„å®ä½“
                entities_i = events[i].get('involved_entities', [])
                entities_j = events[j].get('involved_entities', [])
                
                if isinstance(entities_i, str):
                    try:
                        entities_i = json.loads(entities_i)
                    except:
                        entities_i = []
                
                if isinstance(entities_j, str):
                    try:
                        entities_j = json.loads(entities_j)
                    except:
                        entities_j = []
                
                # æå–å®ä½“åç§°
                names_i = set()
                names_j = set()
                
                for entity in entities_i:
                    if isinstance(entity, dict):
                        names_i.add(entity.get('entity_name', ''))
                
                for entity in entities_j:
                    if isinstance(entity, dict):
                        names_j.add(entity.get('entity_name', ''))
                
                # è®¡ç®— Jaccard ç›¸ä¼¼æ€§
                if names_i or names_j:
                    intersection = len(names_i & names_j)
                    union = len(names_i | names_j)
                    jaccard = intersection / union if union > 0 else 0
                    similarity_matrix[i][j] = jaccard
                    similarity_matrix[j][i] = jaccard
        
        return similarity_matrix
    
    def intelligent_clustering(self, events: List[Dict]) -> List[List[int]]:
        """æ™ºèƒ½å¤šç»´åº¦èšç±»"""
        logger.info(f"ğŸ§  å¼€å§‹æ™ºèƒ½èšç±» {len(events)} ä¸ªäº‹ä»¶...")
        
        if len(events) < 2:
            return [[0]] if events else []
        
        # 1. æå–å¤šç»´åº¦ç‰¹å¾
        structural_features = self.extract_event_features(events)
        semantic_features = self.extract_semantic_features(events)
        entity_similarity = self.calculate_entity_similarity_matrix(events)
        
        # 2. ç»„åˆç‰¹å¾
        # è¯­ä¹‰ç‰¹å¾é™ç»´ï¼ˆPCA æˆ–ç®€å•å¹³å‡ï¼‰
        semantic_reduced = np.mean(semantic_features.reshape(len(events), -1, 32), axis=2)  # ç®€å•é™ç»´
        
        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        combined_features = np.hstack([
            structural_features * self.time_weight,
            semantic_reduced * self.semantic_weight,
        ])
        
        # 3. æ·»åŠ å®ä½“ç›¸ä¼¼æ€§åˆ°è·ç¦»è®¡ç®—ä¸­
        def custom_distance(X):
            """è‡ªå®šä¹‰è·ç¦»å‡½æ•°ï¼Œç»“åˆæ¬§å‡ é‡Œå¾—è·ç¦»å’Œå®ä½“ç›¸ä¼¼æ€§"""
            from sklearn.metrics.pairwise import euclidean_distances
            euclidean_dist = euclidean_distances(X)
            
            # ç»“åˆå®ä½“ç›¸ä¼¼æ€§ï¼ˆç›¸ä¼¼æ€§é«˜çš„äº‹ä»¶è·ç¦»æ›´è¿‘ï¼‰
            entity_dist = 1 - entity_similarity
            
            # åŠ æƒç»„åˆ
            combined_dist = (
                euclidean_dist * (1 - self.entity_weight) +
                entity_dist * self.entity_weight
            )
            
            return combined_dist
        
        # 4. DBSCAN èšç±»
        # ç”±äºè‡ªå®šä¹‰è·ç¦»å‡½æ•°çš„å¤æ‚æ€§ï¼Œå…ˆç”¨æ ‡å‡† DBSCANï¼Œç„¶ååŸºäºå®ä½“ç›¸ä¼¼æ€§è¿›è¡Œåå¤„ç†
        dbscan = DBSCAN(eps=self.dbscan_eps, min_samples=self.dbscan_min_samples)
        cluster_labels = dbscan.fit_predict(combined_features)
        
        # 5. åå¤„ç†ï¼šåˆå¹¶é«˜å®ä½“ç›¸ä¼¼æ€§çš„ç°‡
        clusters = self._post_process_clusters(cluster_labels, entity_similarity, events)
        
        logger.info(f"âœ… èšç±»å®Œæˆ: å‘ç° {len(clusters)} ä¸ªæ•…äº‹ç°‡")
        for i, cluster in enumerate(clusters):
            logger.info(f"   ç°‡ {i}: {len(cluster)} ä¸ªäº‹ä»¶")
        
        return clusters
    
    def _post_process_clusters(self, cluster_labels: np.ndarray, entity_similarity: np.ndarray, events: List[Dict]) -> List[List[int]]:
        """åå¤„ç†èšç±»ç»“æœ"""
        # å°† DBSCAN æ ‡ç­¾è½¬æ¢ä¸ºç°‡åˆ—è¡¨
        clusters = {}
        noise_points = []
        
        for i, label in enumerate(cluster_labels):
            if label == -1:  # å™ªå£°ç‚¹
                noise_points.append(i)
            else:
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append(i)
        
        # å°è¯•å°†å™ªå£°ç‚¹åˆ†é…åˆ°ç›¸ä¼¼çš„ç°‡
        for noise_idx in noise_points:
            best_cluster = None
            best_similarity = 0
            
            for cluster_id, cluster_indices in clusters.items():
                # è®¡ç®—ä¸ç°‡ä¸­æ‰€æœ‰äº‹ä»¶çš„å¹³å‡å®ä½“ç›¸ä¼¼æ€§
                similarities = [entity_similarity[noise_idx][idx] for idx in cluster_indices]
                avg_similarity = np.mean(similarities)
                
                if avg_similarity > best_similarity and avg_similarity > 0.1:  # é˜ˆå€¼
                    best_similarity = avg_similarity
                    best_cluster = cluster_id
            
            if best_cluster is not None:
                clusters[best_cluster].append(noise_idx)
            else:
                # åˆ›å»ºå•ç‹¬çš„ç°‡
                new_cluster_id = max(clusters.keys()) + 1 if clusters else 0
                clusters[new_cluster_id] = [noise_idx]
        
        return list(clusters.values())
    
    def generate_enhanced_story_summary(self, event_indices: List[int], events: List[Dict]) -> str:
        """ç”Ÿæˆå¢å¼ºçš„æ•…äº‹æ‘˜è¦"""
        cluster_events = [events[i] for i in event_indices]
        
        # æå–å…³é”®ä¿¡æ¯
        entities = set()
        raw_dates = []  # ä¿å­˜åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
        event_types = set()
        descriptions = []
        
        for event in cluster_events:
            # å®ä½“
            event_entities = event.get('involved_entities', [])
            if isinstance(event_entities, str):
                try:
                    event_entities = json.loads(event_entities)
                except:
                    event_entities = []
            
            for entity in event_entities:
                if isinstance(entity, dict):
                    entities.add(entity.get('entity_name', ''))
            
            # æ—¶é—´ - ä¿å­˜åŸå§‹å­—ç¬¦ä¸²
            raw_date = event.get('event_date', '')
            if raw_date:
                raw_dates.append(raw_date)
            
            # ç±»å‹
            event_types.add(event.get('assigned_event_type', ''))
            
            # æè¿°
            structured_data = event.get('structured_data', {})
            if isinstance(structured_data, str):
                try:
                    structured_data = json.loads(structured_data)
                except:
                    structured_data = {}
            
            description = structured_data.get('description', '')
            if description:
                descriptions.append(description)
        
        # æ„å»ºæ‘˜è¦
        summary_parts = []
        
        # æ—¶é—´èŒƒå›´ - ä½¿ç”¨åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
        if raw_dates:
            # å»é‡å¹¶æ’åº
            unique_dates = sorted(set(raw_dates))
            if len(unique_dates) == 1:
                time_str = unique_dates[0]
            elif len(unique_dates) == 2:
                time_str = f"{unique_dates[0]} ~ {unique_dates[1]}"
            else:
                time_str = f"{unique_dates[0]} ~ {unique_dates[-1]} (å…±{len(unique_dates)}ä¸ªæ—¶é—´ç‚¹)"
            
            summary_parts.append(f"æ—¶é—´ï¼š{time_str}")
        
        # ä¸»è¦å®ä½“
        if entities:
            entities_list = list(entities)[:5]  # æœ€å¤š5ä¸ªå®ä½“
            summary_parts.append(f"æ¶‰åŠå®ä½“ï¼š{', '.join(entities_list)}")
        
        # äº‹ä»¶ç±»å‹
        if event_types:
            types_list = list(event_types)
            summary_parts.append(f"äº‹ä»¶ç±»å‹ï¼š{', '.join(types_list)}")
        
        # å…³é”®æè¿°
        if descriptions:
            key_description = descriptions[0][:100] + "..." if descriptions[0] else ""
            summary_parts.append(f"å…³é”®äº‹ä»¶ï¼š{key_description}")
        
        return " | ".join(summary_parts)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    agent = EnhancedCortexAgent()
    print("âœ… Enhanced Cortex Agent åˆå§‹åŒ–å®Œæˆ")
