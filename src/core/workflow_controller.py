#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥ä½œæµæ§åˆ¶å™¨

è´Ÿè´£åè°ƒå„æ¨¡å—çš„æ‰§è¡Œé¡ºåºå’Œæ•°æ®ä¼ é€’ï¼Œç®¡ç†ChromaDBå’ŒNeo4jçš„ååŒå·¥ä½œï¼Œ
æ”¯æŒ6é˜¶æ®µæµæ°´çº¿ï¼šæ–‡æœ¬è¾“å…¥ â†’ äº‹ä»¶æŠ½å– â†’ å…³ç³»åˆ†æ â†’ GraphRAGå¢å¼º â†’ å­˜å‚¨ â†’ è¾“å‡º
"""

import asyncio
import logging
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from enum import Enum
from typing import List, Dict, Any, Optional, Callable, Union
from pathlib import Path
import json

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.models.event_data_model import Event
from src.event_logic.hybrid_retriever import HybridRetriever
from src.event_logic.attribute_enhancer import AttributeEnhancer, EnhancedEvent, IncompleteEvent
from src.event_logic.pattern_discoverer import PatternDiscoverer, EventPattern
from src.event_logic.graphrag_coordinator import GraphRAGCoordinator, GraphRAGQuery, GraphRAGResponse


class PipelineStage(Enum):
    """æµæ°´çº¿é˜¶æ®µæšä¸¾"""
    TEXT_INPUT = "text_input"
    EVENT_EXTRACTION = "event_extraction"
    RELATION_ANALYSIS = "relation_analysis"
    GRAPHRAG_ENHANCEMENT = "graphrag_enhancement"
    STORAGE = "storage"
    OUTPUT = "output"


class PipelineStatus(Enum):
    """æµæ°´çº¿çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"


@dataclass
class PipelineConfig:
    """æµæ°´çº¿é…ç½®"""
    # æ•°æ®åº“é…ç½®
    chroma_config: Dict[str, Any] = field(default_factory=dict)
    neo4j_config: Dict[str, Any] = field(default_factory=dict)
    llm_config: Dict[str, Any] = field(default_factory=dict)
    
    # å¤„ç†é…ç½®
    batch_size: int = 100
    max_workers: int = 4
    timeout_seconds: int = 300
    
    # é”™è¯¯å¤„ç†é…ç½®
    max_retries: int = 3
    retry_delay: float = 1.0
    enable_recovery: bool = True
    
    # ç›‘æ§é…ç½®
    enable_monitoring: bool = True
    log_level: str = "INFO"
    
    # è¾“å‡ºé…ç½®
    output_format: str = "jsonl"
    output_path: Optional[str] = None


@dataclass
class StageResult:
    """é˜¶æ®µæ‰§è¡Œç»“æœ"""
    stage: PipelineStage
    status: PipelineStatus
    data: Any = None
    error: Optional[str] = None
    execution_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PipelineResult:
    """æµæ°´çº¿æ‰§è¡Œç»“æœ"""
    pipeline_id: str
    status: PipelineStatus
    stage_results: List[StageResult] = field(default_factory=list)
    total_execution_time: float = 0.0
    processed_items: int = 0
    error_count: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


class DatabaseMonitor:
    """å¢å¼ºçš„æ•°æ®åº“çŠ¶æ€ç›‘æ§å™¨
    
    åŠŸèƒ½:
    - ç›‘æ§ChromaDBå’ŒNeo4jçš„è¿è¡ŒçŠ¶æ€å’Œæ•°æ®åŒæ­¥æƒ…å†µ
    - ç›‘æ§æŒ‡æ ‡å®Œæ•´ï¼Œå¼‚å¸¸å‘Šè­¦åŠæ—¶
    - æ”¯æŒæ•…éšœè‡ªåŠ¨æ¢å¤
    """
    
    def __init__(self, chroma_config: Dict, neo4j_config: Dict):
        self.chroma_config = chroma_config
        self.neo4j_config = neo4j_config
        self.logger = logging.getLogger(__name__)
        self._monitoring = False
        self._monitor_task = None
        
        # è¯¦ç»†ç›‘æ§æŒ‡æ ‡
        self._stats = {
            # ChromaDBçŠ¶æ€
            "chroma_status": "unknown",
            "chroma_response_time": 0.0,
            "chroma_connection_count": 0,
            "chroma_last_error": None,
            "chroma_error_count": 0,
            "chroma_uptime": 0.0,
            
            # Neo4jçŠ¶æ€
            "neo4j_status": "unknown",
            "neo4j_response_time": 0.0,
            "neo4j_connection_count": 0,
            "neo4j_last_error": None,
            "neo4j_error_count": 0,
            "neo4j_uptime": 0.0,
            
            # æ•°æ®åŒæ­¥çŠ¶æ€
            "sync_status": "unknown",
            "sync_lag": 0.0,
            "sync_error_count": 0,
            "last_sync_check": None,
            
            # æ€»ä½“çŠ¶æ€
            "overall_health": "unknown",
            "last_check": None,
            "check_count": 0,
            "alert_count": 0
        }
        
        # å‘Šè­¦é˜ˆå€¼
        self.alert_thresholds = {
            "response_time_ms": 500,  # å“åº”æ—¶é—´é˜ˆå€¼(æ¯«ç§’)
            "error_rate": 0.1,        # é”™è¯¯ç‡é˜ˆå€¼(10%)
            "sync_lag_seconds": 60    # åŒæ­¥å»¶è¿Ÿé˜ˆå€¼(ç§’)
        }
        
        # æ¢å¤ç­–ç•¥é…ç½®
        self.recovery_config = {
            "max_retry_attempts": 3,
            "retry_delay_seconds": 5,
            "circuit_breaker_threshold": 5,
            "recovery_timeout_seconds": 30
        }
        
        # è¿æ¥å®ä¾‹
        self._chroma_client = None
        self._neo4j_driver = None
        
        # å¯åŠ¨æ—¶é—´
        self._start_time = time.time()
    
    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        if self._monitoring:
            self.logger.warning("ç›‘æ§å·²ç»åœ¨è¿è¡Œä¸­")
            return
            
        self._monitoring = True
        self.logger.info("ğŸ” æ•°æ®åº“ç›‘æ§å·²å¯åŠ¨")
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        await self._init_database_connections()
        
        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        self._monitor_task = asyncio.create_task(self._monitor_loop())
    
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self._monitoring = False
        
        if self._monitor_task:
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass
        
        # å…³é—­æ•°æ®åº“è¿æ¥
        await self._close_database_connections()
        
        self.logger.info("ğŸ” æ•°æ®åº“ç›‘æ§å·²åœæ­¢")
    
    async def _init_database_connections(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            # åˆå§‹åŒ–ChromaDBè¿æ¥
            if self.chroma_config:
                import chromadb
                host = self.chroma_config.get("host", "localhost")
                port = self.chroma_config.get("port", 8000)
                self._chroma_client = chromadb.HttpClient(host=host, port=port)
                self.logger.info(f"âœ… ChromaDBè¿æ¥åˆå§‹åŒ–æˆåŠŸ: {host}:{port}")
        except Exception as e:
            self.logger.error(f"âŒ ChromaDBè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            self._chroma_client = None
        
        try:
            # åˆå§‹åŒ–Neo4jè¿æ¥
            if self.neo4j_config:
                from neo4j import GraphDatabase
                uri = self.neo4j_config.get("uri", "bolt://localhost:7687")
                user = self.neo4j_config.get("user", "neo4j")
                password = self.neo4j_config.get("password", "")
                self._neo4j_driver = GraphDatabase.driver(uri, auth=(user, password))
                self.logger.info(f"âœ… Neo4jè¿æ¥åˆå§‹åŒ–æˆåŠŸ: {uri}")
        except Exception as e:
            self.logger.error(f"âŒ Neo4jè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            self._neo4j_driver = None
    
    async def _close_database_connections(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self._neo4j_driver:
            self._neo4j_driver.close()
            self._neo4j_driver = None
        
        # ChromaDBå®¢æˆ·ç«¯é€šå¸¸ä¸éœ€è¦æ˜¾å¼å…³é—­
        self._chroma_client = None
    
    async def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self._monitoring:
            try:
                start_time = time.time()
                
                # æ‰§è¡Œç›‘æ§æ£€æŸ¥
                await self._check_database_status()
                
                # æ›´æ–°æ£€æŸ¥è®¡æ•°
                self._stats["check_count"] += 1
                
                # è®¡ç®—æ£€æŸ¥è€—æ—¶
                check_duration = time.time() - start_time
                self.logger.debug(f"ç›‘æ§æ£€æŸ¥å®Œæˆï¼Œè€—æ—¶: {check_duration:.2f}s")
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"âŒ ç›‘æ§æ£€æŸ¥å¤±è´¥: {e}")
                await asyncio.sleep(60)  # é”™è¯¯æ—¶å»¶é•¿æ£€æŸ¥é—´éš”
    
    async def _check_database_status(self):
        """æ£€æŸ¥æ•°æ®åº“çŠ¶æ€"""
        # æ£€æŸ¥ChromaDBçŠ¶æ€
        await self._check_chroma_status()
        
        # æ£€æŸ¥Neo4jçŠ¶æ€
        await self._check_neo4j_status()
        
        # æ£€æŸ¥æ•°æ®åŒæ­¥çŠ¶æ€
        await self._check_data_sync()
        
        # æ›´æ–°æ€»ä½“å¥åº·çŠ¶æ€
        self._update_overall_health()
        
        # æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´
        self._stats["last_check"] = time.time()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        await self._check_alerts()
    
    async def _check_chroma_status(self):
        """æ£€æŸ¥ChromaDBçŠ¶æ€"""
        if not self._chroma_client:
            self._stats["chroma_status"] = "disconnected"
            return
        
        try:
            start_time = time.time()
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            self._chroma_client.heartbeat()
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            self._stats["chroma_response_time"] = response_time
            self._stats["chroma_status"] = "healthy"
            self._stats["chroma_uptime"] = time.time() - self._start_time
            
            # é‡ç½®é”™è¯¯è®¡æ•°
            if self._stats["chroma_status"] == "healthy":
                self._stats["chroma_error_count"] = 0
                self._stats["chroma_last_error"] = None
            
            self.logger.debug(f"ChromaDBå¥åº·æ£€æŸ¥é€šè¿‡ï¼Œå“åº”æ—¶é—´: {response_time:.2f}ms")
            
        except Exception as e:
            self._stats["chroma_status"] = "error"
            self._stats["chroma_last_error"] = str(e)
            self._stats["chroma_error_count"] += 1
            self.logger.warning(f"âš ï¸ ChromaDBçŠ¶æ€å¼‚å¸¸: {e}")
            
            # å°è¯•è‡ªåŠ¨æ¢å¤
            if self._stats["chroma_error_count"] >= self.recovery_config["circuit_breaker_threshold"]:
                await self._recover_chroma()
    
    async def _check_neo4j_status(self):
        """æ£€æŸ¥Neo4jçŠ¶æ€"""
        if not self._neo4j_driver:
            self._stats["neo4j_status"] = "disconnected"
            return
        
        try:
            start_time = time.time()
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            with self._neo4j_driver.session() as session:
                result = session.run("RETURN 1 as health_check")
                result.single()
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            self._stats["neo4j_response_time"] = response_time
            self._stats["neo4j_status"] = "healthy"
            self._stats["neo4j_uptime"] = time.time() - self._start_time
            
            # é‡ç½®é”™è¯¯è®¡æ•°
            if self._stats["neo4j_status"] == "healthy":
                self._stats["neo4j_error_count"] = 0
                self._stats["neo4j_last_error"] = None
            
            self.logger.debug(f"Neo4jå¥åº·æ£€æŸ¥é€šè¿‡ï¼Œå“åº”æ—¶é—´: {response_time:.2f}ms")
            
        except Exception as e:
            self._stats["neo4j_status"] = "error"
            self._stats["neo4j_last_error"] = str(e)
            self._stats["neo4j_error_count"] += 1
            self.logger.warning(f"âš ï¸ Neo4jçŠ¶æ€å¼‚å¸¸: {e}")
            
            # å°è¯•è‡ªåŠ¨æ¢å¤
            if self._stats["neo4j_error_count"] >= self.recovery_config["circuit_breaker_threshold"]:
                await self._recover_neo4j()
    
    async def _check_data_sync(self):
        """æ£€æŸ¥æ•°æ®åŒæ­¥çŠ¶æ€"""
        try:
            # æ£€æŸ¥ä¸¤ä¸ªæ•°æ®åº“çš„æ•°æ®ä¸€è‡´æ€§
            # è¿™é‡Œå®ç°å…·ä½“çš„åŒæ­¥æ£€æŸ¥é€»è¾‘
            
            # æ¨¡æ‹ŸåŒæ­¥æ£€æŸ¥
            if (self._stats["chroma_status"] == "healthy" and 
                self._stats["neo4j_status"] == "healthy"):
                self._stats["sync_status"] = "synchronized"
                self._stats["sync_lag"] = 0.0
            else:
                self._stats["sync_status"] = "degraded"
                self._stats["sync_lag"] = 30.0  # æ¨¡æ‹Ÿå»¶è¿Ÿ
            
            self._stats["last_sync_check"] = time.time()
            
        except Exception as e:
            self._stats["sync_status"] = "error"
            self._stats["sync_error_count"] += 1
            self.logger.warning(f"âš ï¸ æ•°æ®åŒæ­¥çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
    
    def _update_overall_health(self):
        """æ›´æ–°æ€»ä½“å¥åº·çŠ¶æ€"""
        chroma_ok = self._stats["chroma_status"] == "healthy"
        neo4j_ok = self._stats["neo4j_status"] == "healthy"
        sync_ok = self._stats["sync_status"] in ["synchronized", "degraded"]
        
        if chroma_ok and neo4j_ok and sync_ok:
            self._stats["overall_health"] = "healthy"
        elif (chroma_ok or neo4j_ok) and sync_ok:
            self._stats["overall_health"] = "degraded"
        else:
            self._stats["overall_health"] = "critical"
    
    async def _check_alerts(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦"""
        alerts = []
        
        # æ£€æŸ¥å“åº”æ—¶é—´å‘Šè­¦
        if self._stats["chroma_response_time"] > self.alert_thresholds["response_time_ms"]:
            alerts.append(f"ChromaDBå“åº”æ—¶é—´è¿‡é•¿: {self._stats['chroma_response_time']:.2f}ms")
        
        if self._stats["neo4j_response_time"] > self.alert_thresholds["response_time_ms"]:
            alerts.append(f"Neo4jå“åº”æ—¶é—´è¿‡é•¿: {self._stats['neo4j_response_time']:.2f}ms")
        
        # æ£€æŸ¥åŒæ­¥å»¶è¿Ÿå‘Šè­¦
        if self._stats["sync_lag"] > self.alert_thresholds["sync_lag_seconds"]:
            alerts.append(f"æ•°æ®åŒæ­¥å»¶è¿Ÿè¿‡é•¿: {self._stats['sync_lag']:.2f}s")
        
        # æ£€æŸ¥é”™è¯¯ç‡å‘Šè­¦
        if self._stats["check_count"] > 0:
            chroma_error_rate = self._stats["chroma_error_count"] / self._stats["check_count"]
            neo4j_error_rate = self._stats["neo4j_error_count"] / self._stats["check_count"]
            
            if chroma_error_rate > self.alert_thresholds["error_rate"]:
                alerts.append(f"ChromaDBé”™è¯¯ç‡è¿‡é«˜: {chroma_error_rate:.2%}")
            
            if neo4j_error_rate > self.alert_thresholds["error_rate"]:
                alerts.append(f"Neo4jé”™è¯¯ç‡è¿‡é«˜: {neo4j_error_rate:.2%}")
        
        # å‘é€å‘Šè­¦
        if alerts:
            self._stats["alert_count"] += len(alerts)
            for alert in alerts:
                self.logger.error(f"ğŸš¨ æ•°æ®åº“å‘Šè­¦: {alert}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§çŠ¶æ€"""
        return self._stats.copy()
    
    def get_detailed_status(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†ç›‘æ§çŠ¶æ€"""
        status = self._stats.copy()
        status.update({
            "alert_thresholds": self.alert_thresholds,
            "recovery_config": self.recovery_config,
            "monitoring_active": self._monitoring,
            "uptime_seconds": time.time() - self._start_time
        })
        return status
    
    async def handle_database_failure(self, database: str, error: Exception):
        """å¤„ç†æ•°æ®åº“æ•…éšœ"""
        self.logger.error(f"ğŸš¨ {database}æ•°æ®åº“æ•…éšœ: {error}")
        
        # å®ç°æ•…éšœè‡ªåŠ¨æ¢å¤é€»è¾‘
        if database == "chroma":
            await self._recover_chroma()
        elif database == "neo4j":
            await self._recover_neo4j()
    
    async def _recover_chroma(self):
        """ChromaDBæ•…éšœæ¢å¤"""
        self.logger.info("ğŸ”§ å°è¯•æ¢å¤ChromaDBè¿æ¥...")
        
        for attempt in range(self.recovery_config["max_retry_attempts"]):
            try:
                # é‡æ–°åˆå§‹åŒ–è¿æ¥
                if self.chroma_config:
                    import chromadb
                    host = self.chroma_config.get("host", "localhost")
                    port = self.chroma_config.get("port", 8000)
                    self._chroma_client = chromadb.HttpClient(host=host, port=port)
                    
                    # æµ‹è¯•è¿æ¥
                    self._chroma_client.heartbeat()
                    
                    self.logger.info(f"âœ… ChromaDBè¿æ¥æ¢å¤æˆåŠŸ (å°è¯• {attempt + 1}/{self.recovery_config['max_retry_attempts']})")
                    self._stats["chroma_error_count"] = 0
                    return True
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ ChromaDBæ¢å¤å¤±è´¥ (å°è¯• {attempt + 1}/{self.recovery_config['max_retry_attempts']}): {e}")
                if attempt < self.recovery_config["max_retry_attempts"] - 1:
                    await asyncio.sleep(self.recovery_config["retry_delay_seconds"])
        
        self.logger.error("âŒ ChromaDBæ¢å¤å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
        return False
    
    async def _recover_neo4j(self):
        """Neo4jæ•…éšœæ¢å¤"""
        self.logger.info("ğŸ”§ å°è¯•æ¢å¤Neo4jè¿æ¥...")
        
        for attempt in range(self.recovery_config["max_retry_attempts"]):
            try:
                # å…³é—­æ—§è¿æ¥
                if self._neo4j_driver:
                    self._neo4j_driver.close()
                
                # é‡æ–°åˆå§‹åŒ–è¿æ¥
                if self.neo4j_config:
                    from neo4j import GraphDatabase
                    uri = self.neo4j_config.get("uri", "bolt://localhost:7687")
                    user = self.neo4j_config.get("user", "neo4j")
                    password = self.neo4j_config.get("password", "")
                    self._neo4j_driver = GraphDatabase.driver(uri, auth=(user, password))
                    
                    # æµ‹è¯•è¿æ¥
                    with self._neo4j_driver.session() as session:
                        result = session.run("RETURN 1 as health_check")
                        result.single()
                    
                    self.logger.info(f"âœ… Neo4jè¿æ¥æ¢å¤æˆåŠŸ (å°è¯• {attempt + 1}/{self.recovery_config['max_retry_attempts']})")
                    self._stats["neo4j_error_count"] = 0
                    return True
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ Neo4jæ¢å¤å¤±è´¥ (å°è¯• {attempt + 1}/{self.recovery_config['max_retry_attempts']}): {e}")
                if attempt < self.recovery_config["max_retry_attempts"] - 1:
                    await asyncio.sleep(self.recovery_config["retry_delay_seconds"])
        
        self.logger.error("âŒ Neo4jæ¢å¤å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
        return False


class WorkflowController:
    """å·¥ä½œæµæ§åˆ¶å™¨
    
    åè°ƒå„æ¨¡å—çš„æ‰§è¡Œé¡ºåºå’Œæ•°æ®ä¼ é€’ï¼Œç®¡ç†ChromaDBå’ŒNeo4jçš„ååŒå·¥ä½œï¼Œ
    æ”¯æŒ6é˜¶æ®µæµæ°´çº¿å¤„ç†ã€‚
    """
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.executor = ThreadPoolExecutor(max_workers=config.max_workers)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
        
        # åˆå§‹åŒ–ç›‘æ§
        self.monitor = DatabaseMonitor(config.chroma_config, config.neo4j_config)
        
        # æµæ°´çº¿çŠ¶æ€
        self._pipelines: Dict[str, PipelineResult] = {}
        self._stage_handlers = self._init_stage_handlers()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_pipelines": 0,
            "successful_pipelines": 0,
            "failed_pipelines": 0,
            "avg_execution_time": 0.0
        }
    
    def _init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        try:
            # 1. LLM Config
            from ..llm_integration.llm_config import LLMConfig, LLMProvider
            llm_config_dict = self.config.llm_config
            if llm_config_dict and llm_config_dict.get("api_key"):
                provider_str = llm_config_dict.get("provider", "deepseek")
                llm_config_dict["provider"] = LLMProvider(provider_str)
                llm_config = LLMConfig(**llm_config_dict)
            else:
                llm_config = LLMConfig.from_env()

            # 2. Event Extractor
            from ..llm_integration.llm_event_extractor import LLMEventExtractor
            self.event_extractor = LLMEventExtractor(config=llm_config)
            self.logger.info("âœ… LLMäº‹ä»¶æŠ½å–å™¨åˆå§‹åŒ–æˆåŠŸ")

            # 3. Relation Analyzer
            from ..event_logic.event_logic_analyzer import EventLogicAnalyzer
            self.relation_analyzer = EventLogicAnalyzer(llm_client=self.event_extractor.client)
            self.logger.info("âœ… äº‹ç†å…³ç³»åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")

            # 4. Neo4j Storage
            from ..storage.neo4j_event_storage import Neo4jEventStorage, Neo4jConfig
            neo4j_config_dict = self.config.neo4j_config
            if neo4j_config_dict:
                if 'user' in neo4j_config_dict and 'username' not in neo4j_config_dict:
                    neo4j_config_dict['username'] = neo4j_config_dict.pop('user')
                neo4j_config = Neo4jConfig(**neo4j_config_dict)
            else:
                neo4j_config = Neo4jConfig.from_env()
            self.neo4j_storage = Neo4jEventStorage(config=neo4j_config)
            if self.neo4j_storage.test_connection():
                self.logger.info("âœ… Neo4jå­˜å‚¨è¿æ¥æˆåŠŸ")
            else:
                self.logger.warning("âš ï¸ Neo4jè¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå­˜å‚¨")
                self.neo4j_storage = None

            # 5. Hybrid Retriever
            chroma_collection = self.config.chroma_config.get("collection_name", "events")
            chroma_persist_dir = self.config.chroma_config.get("persist_directory", "./chroma_db")
            neo4j_uri = self.config.neo4j_config.get("uri")
            neo4j_user = self.config.neo4j_config.get("user") or self.config.neo4j_config.get("username")
            neo4j_password = self.config.neo4j_config.get("password")
            self.hybrid_retriever = HybridRetriever(
                chroma_collection=chroma_collection,
                chroma_persist_dir=chroma_persist_dir,
                neo4j_uri=neo4j_uri,
                neo4j_user=neo4j_user,
                neo4j_password=neo4j_password
            )
            self.logger.info("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")

            # 6. Attribute Enhancer
            self.attribute_enhancer = AttributeEnhancer(self.hybrid_retriever)
            self.logger.info("âœ… å±æ€§è¡¥å……å™¨åˆå§‹åŒ–æˆï¿½ï¿½")

            # 7. Pattern Discoverer
            self.pattern_discoverer = PatternDiscoverer(hybrid_retriever=self.hybrid_retriever)
            self.logger.info("âœ… æ¨¡å¼å‘ç°å™¨åˆå§‹åŒ–æˆåŠŸ")

            # 8. GraphRAG Coordinator
            self.graphrag_enhancer = GraphRAGCoordinator(
                hybrid_retriever=self.hybrid_retriever,
                attribute_enhancer=self.attribute_enhancer,
                pattern_discoverer=self.pattern_discoverer,
                max_workers=self.config.max_workers
            )
            self.logger.info("âœ… GraphRAGåè°ƒå™¨åˆå§‹åŒ–æˆåŠŸ")

            self.logger.info("å·¥ä½œæµç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            self.logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            # è®¾ç½®ä¸ºNoneä»¥ä¾¿åç»­ä½¿ç”¨fallbackæ–¹æ³•
            self.event_extractor = None
            self.relation_analyzer = None
            self.neo4j_storage = None
            self.hybrid_retriever = None
            self.attribute_enhancer = None
            self.pattern_discoverer = None
            self.graphrag_enhancer = None
    
    def _init_stage_handlers(self) -> Dict[PipelineStage, Callable]:
        """åˆå§‹åŒ–é˜¶æ®µå¤„ç†å™¨"""
        return {
            PipelineStage.TEXT_INPUT: self._handle_text_input,
            PipelineStage.EVENT_EXTRACTION: self._handle_event_extraction,
            PipelineStage.RELATION_ANALYSIS: self._handle_relation_analysis,
            PipelineStage.GRAPHRAG_ENHANCEMENT: self._handle_graphrag_enhancement,
            PipelineStage.STORAGE: self._handle_storage,
            PipelineStage.OUTPUT: self._handle_output
        }
    
    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        if self.config.enable_monitoring:
            await self.monitor.start_monitoring()
    
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        await self.monitor.stop_monitoring()
    
    async def execute_pipeline(self, 
                             pipeline_id: str,
                             input_data: Any,
                             stages: Optional[List[PipelineStage]] = None) -> PipelineResult:
        """æ‰§è¡Œæµæ°´çº¿
        
        Args:
            pipeline_id: æµæ°´çº¿ID
            input_data: è¾“å…¥æ•°æ®
            stages: è¦æ‰§è¡Œçš„é˜¶æ®µåˆ—è¡¨ï¼Œé»˜è®¤æ‰§è¡Œæ‰€æœ‰é˜¶æ®µ
        
        Returns:
            PipelineResult: æµæ°´çº¿æ‰§è¡Œç»“æœ
        """
        if stages is None:
            stages = list(PipelineStage)
        
        # åˆ›å»ºæµæ°´çº¿ç»“æœ
        pipeline_result = PipelineResult(
            pipeline_id=pipeline_id,
            status=PipelineStatus.RUNNING
        )
        self._pipelines[pipeline_id] = pipeline_result
        
        start_time = time.time()
        current_data = input_data
        
        try:
            self.logger.info(f"å¼€å§‹æ‰§è¡Œæµæ°´çº¿ {pipeline_id}")
            
            # é€é˜¶æ®µæ‰§è¡Œ
            for stage in stages:
                stage_result = await self._execute_stage(stage, current_data, pipeline_id)
                pipeline_result.stage_results.append(stage_result)
                
                if stage_result.status == PipelineStatus.FAILED:
                    pipeline_result.status = PipelineStatus.FAILED
                    pipeline_result.error_count += 1
                    
                    if not self.config.enable_recovery:
                        break
                    
                    # å°è¯•é”™è¯¯æ¢å¤
                    recovery_result = await self._handle_stage_error(stage, stage_result.error, pipeline_id)
                    if not recovery_result:
                        break
                
                # ä¼ é€’æ•°æ®åˆ°ä¸‹ä¸€é˜¶æ®µ
                current_data = stage_result.data
                pipeline_result.processed_items += 1
            
            # æ£€æŸ¥æœ€ç»ˆçŠ¶æ€
            if pipeline_result.status == PipelineStatus.RUNNING:
                pipeline_result.status = PipelineStatus.COMPLETED
                self.stats["successful_pipelines"] += 1
            else:
                self.stats["failed_pipelines"] += 1
            
        except Exception as e:
            self.logger.error(f"æµæ°´çº¿ {pipeline_id} æ‰§è¡Œå¼‚å¸¸: {e}")
            pipeline_result.status = PipelineStatus.FAILED
            pipeline_result.error_count += 1
            self.stats["failed_pipelines"] += 1
        
        finally:
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            pipeline_result.total_execution_time = time.time() - start_time
            self.stats["total_pipelines"] += 1
            self._update_avg_execution_time(pipeline_result.total_execution_time)
            
            self.logger.info(
                f"æµæ°´çº¿ {pipeline_id} æ‰§è¡Œå®Œæˆï¼ŒçŠ¶æ€: {pipeline_result.status.value}, "
                f"è€—æ—¶: {pipeline_result.total_execution_time:.2f}s"
            )
        
        return pipeline_result
    
    async def _execute_stage(self, stage: PipelineStage, data: Any, pipeline_id: str) -> StageResult:
        """æ‰§è¡Œå•ä¸ªé˜¶æ®µ"""
        start_time = time.time()
        
        try:
            self.logger.debug(f"æ‰§è¡Œé˜¶æ®µ {stage.value} (æµæ°´çº¿: {pipeline_id})")
            
            # è·å–é˜¶æ®µå¤„ç†å™¨
            handler = self._stage_handlers.get(stage)
            if not handler:
                raise ValueError(f"æœªæ‰¾åˆ°é˜¶æ®µ {stage.value} çš„å¤„ç†å™¨")
            
            # æ‰§è¡Œé˜¶æ®µå¤„ç†
            result_data = await handler(data, pipeline_id)
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status=PipelineStatus.COMPLETED,
                data=result_data,
                execution_time=execution_time
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.error(f"é˜¶æ®µ {stage.value} æ‰§è¡Œå¤±è´¥: {e}")
            
            return StageResult(
                stage=stage,
                status=PipelineStatus.FAILED,
                error=str(e),
                execution_time=execution_time
            )
    
    async def _handle_text_input(self, data: Any, pipeline_id: str) -> Dict[str, Any]:
        """å¤„ç†æ–‡æœ¬è¾“å…¥é˜¶æ®µ"""
        if isinstance(data, str):
            return {
                "text": data,
                "pipeline_id": pipeline_id,
                "timestamp": time.time()
            }
        elif isinstance(data, dict) and "text" in data:
            data["pipeline_id"] = pipeline_id
            data["timestamp"] = time.time()
            return data
        else:
            raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–åŒ…å«'text'å­—æ®µçš„å­—å…¸")
    
    async def _handle_event_extraction(self, data: Dict[str, Any], pipeline_id: str) -> List[Event]:
        """å¤„ç†äº‹ä»¶æŠ½å–é˜¶æ®µ"""
        text = data.get("text", "")
        
        if not text:
            self.logger.warning("è¾“å…¥æ–‡æœ¬ä¸ºç©º")
            return []
        
        try:
            if self.event_extractor:
                # ä½¿ç”¨å®é™…çš„LLMäº‹ä»¶æŠ½å–å™¨
                self.logger.info(f"å¼€å§‹æŠ½å–äº‹ä»¶ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}")
                
                result = self.event_extractor.extract_events(
                    text=text,
                    event_types=["business_cooperation", "personnel_change", "product_launch", "investment", "other"],
                    entity_types=["organization", "person", "product", "location", "other"]
                )
                
                if result.success:
                    events = result.events
                    # ä¸ºæ¯ä¸ªäº‹ä»¶æ·»åŠ pipeline_id
                    for event in events:
                        if not hasattr(event, 'properties') or event.properties is None:
                            event.properties = {}
                        event.properties["pipeline_id"] = pipeline_id
                        event.properties["source"] = "llm_extraction"
                    
                    self.logger.info(f"âœ… æˆåŠŸæŠ½å–åˆ° {len(events)} ä¸ªäº‹ä»¶ï¼Œå¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
                    return events
                else:
                    self.logger.error(f"äº‹ä»¶æŠ½å–å¤±è´¥: {result.error_message}")
                    return self._create_fallback_events(text, pipeline_id)
            else:
                # ä½¿ç”¨fallbackæ–¹æ³•
                self.logger.warning("äº‹ä»¶æŠ½å–å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨fallbackæ–¹æ³•")
                return self._create_fallback_events(text, pipeline_id)
                
        except Exception as e:
            self.logger.error(f"äº‹ä»¶æŠ½å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return self._create_fallback_events(text, pipeline_id)
    
    def _create_fallback_events(self, text: str, pipeline_id: str) -> List[Event]:
        """åˆ›å»ºfallbackäº‹ä»¶ï¼ˆå½“LLMæŠ½å–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        from ..models.event_data_model import Event, EventType
        
        events = [
            Event(
                id=f"event_{pipeline_id}_fallback_001",
                event_type=EventType.OTHER,
                text=text,
                summary=f"ä»æ–‡æœ¬ä¸­æŠ½å–çš„äº‹ä»¶: {text[:100]}...",
                timestamp=str(time.time()),
                properties={"source": "fallback_extraction", "pipeline_id": pipeline_id}
            )
        ]
        
        self.logger.info(f"åˆ›å»ºäº† {len(events)} ä¸ªfallbackäº‹ä»¶")
        return events
    
    async def _handle_relation_analysis(self, events: List[Event], pipeline_id: str) -> Dict[str, Any]:
        """å¤„ç†å…³ç³»åˆ†æé˜¶æ®µ"""
        if len(events) < 2:
            self.logger.info(f"äº‹ä»¶æ•°é‡ä¸è¶³({len(events)})ï¼Œè·³è¿‡å…³ç³»åˆ†æ")
            return {
                "events": events,
                "relations": [],
                "pipeline_id": pipeline_id
            }
        
        try:
            if self.relation_analyzer:
                # ä½¿ç”¨å®é™…çš„äº‹ç†å…³ç³»åˆ†æå™¨
                self.logger.info(f"å¼€å§‹åˆ†æ {len(events)} ä¸ªäº‹ä»¶çš„å…³ç³»")
                
                relations = self.relation_analyzer.analyze_event_relations(events)
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä»¥ä¾¿åºåˆ—åŒ–
                relations_dict = []
                for relation in relations:
                    relations_dict.append({
                        "id": getattr(relation, 'id', f"rel_{len(relations_dict)}"),
                        "source_event": relation.source_event_id,
                        "target_event": relation.target_event_id,
                        "relation_type": relation.relation_type.value if hasattr(relation.relation_type, 'value') else str(relation.relation_type),
                        "confidence": relation.confidence,
                        "strength": getattr(relation, 'strength', 0.0),
                        "description": getattr(relation, 'description', ''),
                        "evidence": getattr(relation, 'evidence', ''),
                        "source": getattr(relation, 'source', 'analyzer'),
                        "pipeline_id": pipeline_id
                    })
                
                result = {
                    "events": events,
                    "relations": relations_dict,
                    "pipeline_id": pipeline_id
                }
                
                self.logger.info(f"âœ… æˆåŠŸåˆ†æäº† {len(events)} ä¸ªäº‹ä»¶ï¼Œå‘ç° {len(relations_dict)} ä¸ªå…³ç³»")
                return result
            else:
                # ä½¿ç”¨fallbackæ–¹æ³•
                self.logger.warning("å…³ç³»åˆ†æå™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨fallbackæ–¹æ³•")
                return self._create_fallback_relations(events, pipeline_id)
                
        except Exception as e:
            self.logger.error(f"å…³ç³»åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return self._create_fallback_relations(events, pipeline_id)
    
    def _create_fallback_relations(self, events: List[Event], pipeline_id: str) -> Dict[str, Any]:
        """åˆ›å»ºfallbackå…³ç³»ï¼ˆå½“å…³ç³»åˆ†æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        relations = []
        
        # ç®€å•çš„æ—¶åºå…³ç³»æ¨æ–­
        for i in range(len(events) - 1):
            relations.append({
                "id": f"rel_fallback_{i}",
                "source_event": events[i].id,
                "target_event": events[i + 1].id,
                "relation_type": "temporal_sequence",
                "confidence": 0.5,
                "strength": 0.4,
                "description": "åŸºäºé¡ºåºçš„æ—¶åºå…³ç³»",
                "evidence": "äº‹ä»¶é¡ºåºæ¨æ–­",
                "source": "fallback_analysis",
                "pipeline_id": pipeline_id
            })
        
        result = {
            "events": events,
            "relations": relations,
            "pipeline_id": pipeline_id
        }
        
        self.logger.info(f"åˆ›å»ºäº† {len(relations)} ä¸ªfallbackå…³ç³»")
        return result
    
    async def _handle_graphrag_enhancement(self, data: Dict[str, Any], pipeline_id: str) -> Dict[str, Any]:
        """å¤„ç†GraphRAGå¢å¼ºé˜¶æ®µ"""
        events = data.get("events", [])
        
        # åˆ›å»ºGraphRAGæŸ¥è¯¢
        query = GraphRAGQuery(
            query_id=f"enhancement_{pipeline_id}",
            query_text="å¢å¼ºäº‹ä»¶ä¿¡æ¯",
            query_type="hybrid",
            target_events=events,
            parameters={
                "top_k": 10,
                "enhance_attributes": True,
                "discover_patterns": True
            }
        )
        
        # æ‰§è¡ŒGraphRAGå¢å¼º
        response = await self.graphrag_enhancer.process_query(query)
        
        # æ•´åˆç»“æœ
        enhanced_data = {
            "original_events": events,
            "enhanced_events": response.enhanced_events or [],
            "discovered_patterns": response.discovered_patterns or [],
            "retrieved_events": response.retrieved_events or [],
            "relations": data.get("relations", []),
            "confidence_scores": response.confidence_scores,
            "pipeline_id": pipeline_id
        }
        
        self.logger.info(f"GraphRAGå¢å¼ºå®Œæˆï¼Œå¢å¼ºäº‹ä»¶: {len(enhanced_data['enhanced_events'])}")
        return enhanced_data
    
    async def _handle_storage(self, data: Dict[str, Any], pipeline_id: str) -> Dict[str, Any]:
        """å¤„ç†å­˜å‚¨é˜¶æ®µ"""
        try:
            # å­˜å‚¨åˆ°ChromaDBå’ŒNeo4j
            events = data.get("enhanced_events", []) or data.get("original_events", [])
            patterns = data.get("discovered_patterns", [])
            relations = data.get("relations", [])
            
            # å­˜å‚¨äº‹ä»¶
            if events:
                await self._store_events(events)
            
            # å­˜å‚¨æ¨¡å¼
            if patterns:
                await self._store_patterns(patterns)
            
            # å­˜å‚¨å…³ç³»
            if relations:
                await self._store_relations(relations)
            
            storage_result = {
                "stored_events": len(events),
                "stored_patterns": len(patterns),
                "stored_relations": len(relations),
                "pipeline_id": pipeline_id,
                "data": data  # ä¼ é€’åŸå§‹æ•°æ®åˆ°è¾“å‡ºé˜¶æ®µ
            }
            
            self.logger.info(f"å­˜å‚¨å®Œæˆ: äº‹ä»¶{len(events)}ä¸ª, æ¨¡å¼{len(patterns)}ä¸ª, å…³ç³»{len(relations)}ä¸ª")
            return storage_result
            
        except Exception as e:
            self.logger.error(f"å­˜å‚¨é˜¶æ®µå¤±è´¥: {e}")
            raise
    
    async def _handle_output(self, data: Dict[str, Any], pipeline_id: str) -> Dict[str, Any]:
        """å¤„ç†è¾“å‡ºé˜¶æ®µ"""
        # æ ¼å¼åŒ–è¾“å‡ºæ•°æ®
        output_data = {
            "pipeline_id": pipeline_id,
            "timestamp": time.time(),
            "events": data.get("data", {}).get("enhanced_events", []) or data.get("data", {}).get("original_events", []),
            "patterns": data.get("data", {}).get("discovered_patterns", []),
            "relations": data.get("data", {}).get("relations", []),
            "confidence_scores": data.get("data", {}).get("confidence_scores", {}),
            "storage_stats": {
                "stored_events": data.get("stored_events", 0),
                "stored_patterns": data.get("stored_patterns", 0),
                "stored_relations": data.get("stored_relations", 0)
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚æœé…ç½®äº†è¾“å‡ºè·¯å¾„ï¼‰
        if self.config.output_path:
            await self._save_output(output_data, pipeline_id)
        
        self.logger.info(f"è¾“å‡ºé˜¶æ®µå®Œæˆï¼Œæµæ°´çº¿ {pipeline_id}")
        return output_data
    
    async def _store_events(self, events: List[Event]):
        """å­˜å‚¨äº‹ä»¶åˆ°æ•°æ®åº“"""
        if not events:
            return
        
        try:
            if self.neo4j_storage:
                # ä½¿ç”¨å®é™…çš„Neo4jå­˜å‚¨
                for event in events:
                    success = self.neo4j_storage.store_event(event)
                    if not success:
                        self.logger.warning(f"äº‹ä»¶ {event.id} å­˜å‚¨å¤±è´¥")
                
                self.logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {len(events)} ä¸ªäº‹ä»¶åˆ°Neo4j")
            else:
                # æ¨¡æ‹Ÿå­˜å‚¨
                self.logger.warning(f"âš ï¸ Neo4jå­˜å‚¨ä¸å¯ç”¨ï¼Œæ¨¡æ‹Ÿå­˜å‚¨ {len(events)} ä¸ªäº‹ä»¶")
                
        except Exception as e:
            self.logger.error(f"å­˜å‚¨äº‹ä»¶å¤±è´¥: {e}")
    
    async def _store_patterns(self, patterns: List[EventPattern]):
        """å­˜å‚¨æ¨¡å¼åˆ°æ•°æ®åº“"""
        # è¿™é‡Œåº”è¯¥å®é™…å­˜å‚¨åˆ°ChromaDBå’ŒNeo4j
        pass
    
    async def _store_relations(self, relations: List[Dict]):
        """å­˜å‚¨å…³ç³»åˆ°æ•°æ®åº“"""
        if not relations:
            return
        
        try:
            if self.neo4j_storage:
                # ä½¿ç”¨å®é™…çš„Neo4jå­˜å‚¨
                for relation in relations:
                    success = self.neo4j_storage.store_relation(
                        source_event_id=relation["source_event"],
                        target_event_id=relation["target_event"],
                        relation_type=relation["relation_type"],
                        properties={
                            "confidence": relation.get("confidence", 0.0),
                            "strength": relation.get("strength", 0.0),
                            "description": relation.get("description", ""),
                            "evidence": relation.get("evidence", ""),
                            "source": relation.get("source", "unknown"),
                            "pipeline_id": relation.get("pipeline_id", "")
                        }
                    )
                    if not success:
                        self.logger.warning(f"å…³ç³» {relation.get('id', 'unknown')} å­˜å‚¨å¤±è´¥")
                
                self.logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {len(relations)} ä¸ªå…³ç³»åˆ°Neo4j")
            else:
                # æ¨¡æ‹Ÿå­˜å‚¨
                self.logger.warning(f"âš ï¸ Neo4jå­˜å‚¨ä¸å¯ç”¨ï¼Œæ¨¡æ‹Ÿå­˜å‚¨ {len(relations)} ä¸ªå…³ç³»")
                
        except Exception as e:
            self.logger.error(f"å­˜å‚¨å…³ç³»å¤±è´¥: {e}")
    
    async def _save_output(self, data: Dict[str, Any], pipeline_id: str):
        """ä¿å­˜è¾“å‡ºåˆ°æ–‡ä»¶"""
        try:
            output_path = Path(self.config.output_path)
            output_path.mkdir(parents=True, exist_ok=True)
            
            if self.config.output_format == "jsonl":
                file_path = output_path / f"{pipeline_id}.jsonl"
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"è¾“å‡ºå·²ä¿å­˜åˆ°: {file_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¾“å‡ºå¤±è´¥: {e}")
    
    async def _handle_stage_error(self, stage: PipelineStage, error: str, pipeline_id: str) -> bool:
        """å¤„ç†é˜¶æ®µé”™è¯¯"""
        self.logger.warning(f"å°è¯•æ¢å¤é˜¶æ®µ {stage.value} çš„é”™è¯¯: {error}")
        
        # å®ç°é”™è¯¯æ¢å¤é€»è¾‘
        # è¿™é‡Œå¯ä»¥æ ¹æ®ä¸åŒçš„é”™è¯¯ç±»å‹å®ç°ä¸åŒçš„æ¢å¤ç­–ç•¥
        
        return False  # æš‚æ—¶è¿”å›Falseï¼Œè¡¨ç¤ºæ— æ³•æ¢å¤
    
    def _update_avg_execution_time(self, execution_time: float):
        """æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´"""
        total = self.stats["total_pipelines"]
        if total > 0:
            current_avg = self.stats["avg_execution_time"]
            self.stats["avg_execution_time"] = (current_avg * (total - 1) + execution_time) / total
    
    def get_pipeline_status(self, pipeline_id: str) -> Optional[PipelineResult]:
        """è·å–æµæ°´çº¿çŠ¶æ€"""
        return self._pipelines.get(pipeline_id)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "database_status": self.monitor.get_status(),
            "active_pipelines": len([p for p in self._pipelines.values() if p.status == PipelineStatus.RUNNING])
        }
    
    async def batch_execute(self, 
                          input_data_list: List[Any],
                          pipeline_prefix: str = "batch") -> List[PipelineResult]:
        """æ‰¹é‡æ‰§è¡Œæµæ°´çº¿"""
        tasks = []
        
        for i, input_data in enumerate(input_data_list):
            pipeline_id = f"{pipeline_prefix}_{i:04d}"
            task = self.execute_pipeline(pipeline_id, input_data)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        pipeline_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                pipeline_id = f"{pipeline_prefix}_{i:04d}"
                error_result = PipelineResult(
                    pipeline_id=pipeline_id,
                    status=PipelineStatus.FAILED
                )
                error_result.error_count = 1
                pipeline_results.append(error_result)
            else:
                pipeline_results.append(result)
        
        return pipeline_results
    
    async def shutdown(self):
        """å…³é—­å·¥ä½œæµæ§åˆ¶å™¨"""
        self.logger.info("æ­£åœ¨å…³é—­å·¥ä½œæµæ§åˆ¶å™¨...")
        
        # åœæ­¢ç›‘æ§
        await self.stop_monitoring()
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=True)
        
        # å…³é—­ç»„ä»¶è¿æ¥
        if self.hybrid_retriever and hasattr(self.hybrid_retriever, 'close'):
            self.hybrid_retriever.close()
        
        self.logger.info("å·¥ä½œæµæ§åˆ¶å™¨å·²å…³é—­")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    async def main():
        # é…ç½®
        config = PipelineConfig(
            chroma_config={"host": "localhost", "port": 8000},
            neo4j_config={"uri": "bolt://localhost:7687", "user": "neo4j", "password": "neo123456"},
            batch_size=50,
            max_workers=2,
            enable_monitoring=True,
            output_path="./output"
        )
        
        # åˆ›å»ºå·¥ä½œæµæ§åˆ¶å™¨
        controller = WorkflowController(config)
        
        try:
            # å¯åŠ¨ç›‘æ§
            await controller.start_monitoring()
            
            # æ‰§è¡Œå•ä¸ªæµæ°´çº¿
            result = await controller.execute_pipeline(
                pipeline_id="test_001",
                input_data="è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºæ¼”ç¤ºäº‹ä»¶æŠ½å–å’Œå¤„ç†æµç¨‹ã€‚"
            )
            
            print(f"æµæ°´çº¿æ‰§è¡Œç»“æœ: {result.status.value}")
            print(f"æ‰§è¡Œæ—¶é—´: {result.total_execution_time:.2f}s")
            print(f"å¤„ç†é¡¹ç›®æ•°: {result.processed_items}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = controller.get_statistics()
            print(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")
            
        finally:
            # å…³é—­æ§åˆ¶å™¨
            await controller.shutdown()
    
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())