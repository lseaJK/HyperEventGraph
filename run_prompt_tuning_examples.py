# run_prompt_tuning_examples.py
"""
This script generates concrete examples for each of the three core LLM-driven
tasks in the HyperEventGraph system: Triage, Schema Generation, and Extraction.

It is designed to be run after the system has been configured to use a new
LLM provider, allowing developers to see the exact prompts and the model's
raw JSON output for tuning and evaluation purposes.
"""
import asyncio
import json
from pathlib import Path
import sys

# Add project root to sys.path
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

from src.core.config_loader import load_config, get_config
from src.llm.llm_client import LLMClient
from src.agents.toolkits.schema_learning_toolkit import SchemaLearningToolkit
from src.event_extraction.deepseek_extractor import DeepSeekEventExtractor
from src.event_extraction.schemas import get_event_model, EVENT_SCHEMA_REGISTRY

# --- Sample Data for Each Scenario ---

TRIAGE_TEXT = "ã€Šç§‘åˆ›æ¿æ—¥æŠ¥ã€‹24æ—¥è®¯ï¼Œæ™¶åœ†ä»£å·¥ä¸šä¸‹åŠå¹´å±•æœ›é»¯æ·¡ï¼ŒICè®¾è®¡ä¸šè€…é€éœ²ï¼Œç›®å‰é™¤äº†å°ç§¯ç”µä»åšå®ˆä»·æ ¼ä¹‹å¤–ï¼Œå…¶ä»–æ™¶åœ†ä»£å·¥å‚éƒ½å·²æœ‰ä¸åŒç¨‹åº¦ä¸å½¢å¼é™ä»·ã€‚"

SCHEMA_GEN_SAMPLES = [
    "è´¢è”ç¤¾7æœˆ21æ—¥ç”µï¼ŒAMDè‘£äº‹é•¿å…¼CEOè‹å§¿ä¸°ä»Šæ—¥è¡¨ç¤ºï¼Œé™¤äº†å°ç§¯ç”µï¼ŒAMDè¿˜å°†è€ƒè™‘ç”±å…¶ä»–ä»£å·¥å‚å•†æ¥ç”Ÿäº§AMDè®¾è®¡çš„èŠ¯ç‰‡ï¼Œä»¥ç¡®ä¿ä¾›åº”é“¾çš„å¼¹æ€§ã€‚",
    "ã€Šç§‘åˆ›æ¿æ—¥æŠ¥ã€‹20æ—¥è®¯ï¼Œæ®éŸ©åª’æŠ¥é“ï¼Œæœ‰ä¸šå†…äººå£«é€éœ²ï¼Œä¸‰æ˜Ÿç”µå­å°†ä¸ºç‰¹æ–¯æ‹‰HW 5.0ç”Ÿäº§æ–°ä¸€ä»£FSDèŠ¯ç‰‡ã€‚è¯¥èŠ¯ç‰‡å°†é‡‡ç”¨ä¸‰æ˜Ÿ4nmå·¥è‰ºåˆ¶ç¨‹ã€‚",
    "ã€Šç§‘åˆ›æ¿æ—¥æŠ¥ã€‹13æ—¥è®¯ï¼ŒIBMæ€»ç»ç†å¡å‹’å—è®¿è¡¨ç¤ºï¼Œæ——ä¸‹æ–°ä¸€ä»£ä¼ä¸šçº§AIæ•°æ®å¹³å°â€œWatsonâ€ç³»ç»Ÿå°†é‡‡ç”¨è‡ªç ”AIèŠ¯ç‰‡ï¼Œå¹¶äº¤ç”±ä¸‰æ˜Ÿä»£å·¥ã€‚"
]

EXTRACTION_TEXT = "è´¢è”ç¤¾7æœˆ17æ—¥ç”µï¼Œä¸­èŠ¯å›½é™…(00981.HK)å‘å¸ƒå…¬å‘Šï¼Œé«˜æ°¸å²—å› å·¥ä½œè°ƒæ•´ï¼Œè¾ä»»å…¬å¸è‘£äº‹é•¿ã€æ‰§è¡Œè‘£äº‹åŠè‘£äº‹ä¼šæåå§”å‘˜ä¼šä¸»å¸­èŒåŠ¡ï¼›å…¬å¸å‰¯è‘£äº‹é•¿åˆ˜è®­å³°åšå£«è·å§”ä»»ä¸ºå…¬å¸è‘£äº‹é•¿ã€‚"
EXTRACTION_SCHEMA_NAME = "Company:ExecutiveChange"

# --- Helper to print formatted sections ---

def print_section(title, prompt, result):
    print("\n" + "="*80)
    print(f"ğŸ¬ SCENARIO: {title}")
    print("="*80)
    print("\n--- PROMPT SENT TO LLM ---\n")
    print(prompt)
    print("\n--- LLM JSON RESPONSE ---\n")
    try:
        # Pretty-print the JSON response
        parsed_json = json.loads(result)
        print(json.dumps(parsed_json, indent=2, ensure_ascii=False))
    except (json.JSONDecodeError, TypeError):
        print(result if result else "None")
    print("\n" + "="*80)

async def run_triage_example(llm_client: LLMClient):
    """Generates an example for the Triage task."""
    # This simulates the TriageAgent's core logic
    event_types_str = "\n- ".join(list(EVENT_SCHEMA_REGISTRY.keys()))
    domains_str = "\n- ".join(["financial", "circuit", "general"]) # Example domains
    
    prompt = f"""
You are a Triage Agent responsible for classifying event types and their domains.

CRITICAL INSTRUCTIONS:
1. You MUST output ONLY a valid JSON object - nothing else.
2. DO NOT include any explanatory text before or after the JSON.
3. DO NOT use XML tags, markdown, or any other formatting.
4. DO NOT mention tools or function calls.

Your output format MUST be exactly:
{{"status": "known", "domain": "äº‹ä»¶é¢†åŸŸ", "event_type": "äº‹ä»¶ç±»å‹", "confidence": 0.95}}

æˆ–è€…:
{{"status": "unknown", "event_type": "Unknown", "domain": "unknown", "confidence": 0.99}}

IMPORTANT: If you output anything other than a pure JSON object, the system will fail.

Here are the domains you can recognize:
- {domains_str}

Here are the event types you can recognize:
- {event_types_str}

Analyze the provided text, determine the most appropriate domain and event type, and then output ONLY the JSON classification.

--- TEXT TO ANALYZE ---
{TRIAGE_TEXT}
"""
    result = await llm_client.get_raw_response(prompt, task_type="triage")
    print_section("1. Batch Triage", prompt, result)

async def run_schema_gen_example(llm_client: LLMClient):
    """Generates an example for the Schema Generation task."""
    # This simulates the SchemaLearningToolkit's core logic
    toolkit = SchemaLearningToolkit(db_path=":memory:") # Use in-memory to avoid real DB dependency
    prompt = toolkit._build_schema_generation_prompt(SCHEMA_GEN_SAMPLES)
    
    result = await llm_client.get_raw_response(prompt, task_type="schema_generation")
    print_section("2. Schema Generation", prompt, result)

async def run_extraction_example():
    """Generates an example for the Event Extraction task."""
    # This simulates the DeepSeekEventExtractor's core logic
    extractor = DeepSeekEventExtractor()
    EventModel = get_event_model(EXTRACTION_SCHEMA_NAME)
    
    if not EventModel:
        print(f"Could not find schema for '{EXTRACTION_SCHEMA_NAME}'")
        return

    json_schema = EventModel.schema()
    prompt = extractor.template_generator.generate_prompt(
        text=EXTRACTION_TEXT,
        event_schema=json_schema
    )
    
    # We call the client directly to get the raw response for inspection
    result = await extractor.client.chat.completions.create(
        model=extractor.config.model_name,
        messages=[{"role": "user", "content": prompt}],
        temperature=extractor.config.temperature,
        max_tokens=extractor.config.max_tokens,
        response_format={"type": "json_object"},
    )
    content = result.choices[0].message.content
    print_section("3. Event Extraction", prompt, content)


async def main():
    """Main function to run all examples."""
    print("Loading configuration from config.yaml...")
    load_config("config.yaml")
    
    # A single LLM client is used, which routes to the correct model via task_type
    llm_client = LLMClient()

    await run_triage_example(llm_client)
    await run_schema_gen_example(llm_client)
    await run_extraction_example()
    
    print("\nâœ… All examples generated. Please review the prompts and responses for tuning.")

if __name__ == "__main__":
    asyncio.run(main())
